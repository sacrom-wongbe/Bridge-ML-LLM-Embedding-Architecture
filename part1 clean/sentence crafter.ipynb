{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5746e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import random\n",
    "\n",
    "def print_section(title, char=\"=\", width=80):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(title.center(width))\n",
    "    print(f\"{char * width}\\n\")\n",
    "\n",
    "def find_column_index(df: pd.DataFrame, column_name: str) -> int:\n",
    "    \"\"\"Find the index of a column by name.\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df.columns.get_loc(column_name)\n",
    "    raise ValueError(f\"Column '{column_name}' not found in dataframe\")\n",
    "\n",
    "def get_numerical_bin_sentence(value: float, edges: List, sentences: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find which bin the value falls into and return corresponding sentence.\n",
    "    edges = [0, 20, 50, 80, \"inf\"] creates 4 bins:\n",
    "      - bin 0: [0, 20)\n",
    "      - bin 1: [20, 50)\n",
    "      - bin 2: [50, 80)\n",
    "      - bin 3: [80, inf)\n",
    "    \"\"\"\n",
    "    # Convert \"inf\" strings to float('inf')\n",
    "    edges_numeric = []\n",
    "    for e in edges:\n",
    "        if isinstance(e, str) and e.lower() in ['inf', '-inf']:\n",
    "            edges_numeric.append(float(e))\n",
    "        else:\n",
    "            edges_numeric.append(float(e))\n",
    "    \n",
    "    # Find bin index\n",
    "    bin_index = 0\n",
    "    for i in range(len(edges_numeric) - 1):\n",
    "        if value >= edges_numeric[i] and value < edges_numeric[i + 1]:\n",
    "            bin_index = i\n",
    "            break\n",
    "        elif i == len(edges_numeric) - 2:  # Last bin (to infinity)\n",
    "            bin_index = i\n",
    "    \n",
    "    return sentences[bin_index]\n",
    "\n",
    "def ensure_period(text: str) -> str:\n",
    "    \"\"\"Ensure text ends with a period.\"\"\"\n",
    "    text = text.strip()\n",
    "    if text and not text.endswith('.'):\n",
    "        text += '.'\n",
    "    return text\n",
    "\n",
    "def generate_sentence(column_name: str, value: Any, schema_field: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full sentence for a column value based on schema rules.\n",
    "    Returns: \"{title}: {sentence}\"\n",
    "    \"\"\"\n",
    "    # Skip null/missing values\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    title = schema_field.get('title', column_name)\n",
    "    field_type = schema_field.get('type')\n",
    "    \n",
    "    if field_type == 'nominal':\n",
    "        # Match value to code_map\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        if sentence:\n",
    "            return f\"{title}: {ensure_period(sentence)}\"\n",
    "    \n",
    "    elif field_type == 'nl':\n",
    "        # Use raw value as sentence\n",
    "        return f\"{title}: {ensure_period(str(value))}\"\n",
    "    \n",
    "    elif field_type == 'numerical':\n",
    "        # Use semantic bins\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        sentences = semantic_bins.get('sentences', [])\n",
    "        \n",
    "        if edges and sentences:\n",
    "            try:\n",
    "                sentence = get_numerical_bin_sentence(float(value), edges, sentences)\n",
    "                return f\"{title}: {ensure_period(sentence)}\"\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    elif field_type == 'numerical_coded':\n",
    "        # Try code_map first, fallback to semantic bins\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        \n",
    "        if sentence:\n",
    "            return f\"{title}: {ensure_period(sentence)}\"\n",
    "        \n",
    "        # Fallback to semantic bins\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        sentences = semantic_bins.get('sentences', [])\n",
    "        \n",
    "        if edges and sentences:\n",
    "            try:\n",
    "                sentence = get_numerical_bin_sentence(float(value), edges, sentences)\n",
    "                return f\"{title}: {ensure_period(sentence)}\"\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical_coded value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_schema_csv_pairs(schema_folder: Path, data_folder: Path) -> List[Tuple[Dict, pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Load matching schema/CSV pairs.\n",
    "    Returns: List of (schema_dict, dataframe, source_name) tuples\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Get all schema files\n",
    "    schema_files = sorted(schema_folder.glob(\"*_schema_master.json\"))\n",
    "    \n",
    "    for schema_file in schema_files:\n",
    "        # Derive CSV filename\n",
    "        csv_name = schema_file.stem.replace(\"_schema_master\", \"\") + \".csv\"\n",
    "        csv_path = data_folder / csv_name\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"âš ï¸  Warning: No matching CSV for {schema_file.name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load schema\n",
    "        with open(schema_file, 'r', encoding='utf-8') as f:\n",
    "            schema = json.load(f)\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        source_name = csv_name.replace(\".csv\", \"\")\n",
    "        pairs.append((schema, df, source_name))\n",
    "        print(f\"âœ… Loaded: {schema_file.name} + {csv_name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_paragraph_for_row(row_index: int, schema_csv_pairs: List[Tuple[Dict, pd.DataFrame, str]]) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generate a paragraph for a single row across all CSVs.\n",
    "    Returns: (structure_id, coordinates, paragraph)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    structure_id = None\n",
    "    coordinates = None\n",
    "    \n",
    "    for schema, df, source_name in schema_csv_pairs:\n",
    "        row = df.iloc[row_index]\n",
    "        \n",
    "        # Extract structure_id and coordinates from first pair\n",
    "        if structure_id is None:\n",
    "            structure_id = row.iloc[0] if 'STRUCTURE_ID' not in df.columns else row['STRUCTURE_ID']\n",
    "        if coordinates is None:\n",
    "            coordinates = row.iloc[1] if 'COORDINATES' not in df.columns else row['COORDINATES']\n",
    "        \n",
    "        # Process each column (skip STRUCTURE_ID and COORDINATES)\n",
    "        for column_name, schema_field in schema.items():\n",
    "            if column_name in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "                continue\n",
    "            \n",
    "            # Skip reference and redundant types\n",
    "            field_type = schema_field.get('type')\n",
    "            if field_type in ['reference', 'numerical_redundant', 'nominal_redundant', 'numerical_coded_redundant']:\n",
    "                continue\n",
    "            \n",
    "            if column_name not in df.columns:\n",
    "                print(f\"    âš ï¸  Column mismatch: '{column_name}' in schema but not in {source_name}.csv\")\n",
    "                raise ValueError(f\"Column mismatch: {column_name}\")\n",
    "            \n",
    "            value = row[column_name]\n",
    "            sentence = generate_sentence(column_name, value, schema_field)\n",
    "            \n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    # Combine all sentences into one paragraph\n",
    "    paragraph = \" \".join(sentences)\n",
    "    \n",
    "    return str(structure_id), str(coordinates), paragraph\n",
    "\n",
    "def test_random_paragraph(schema_folder: str, data_folder: str, output_folder: str = \"logs\"):\n",
    "    \"\"\"\n",
    "    Generate a paragraph for a random bridge and save to log file.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    print_section(\"TEST MODE - Random Paragraph Generation\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Get row count from first dataframe\n",
    "    num_rows = len(pairs[0][1])\n",
    "    print(f\"\\nğŸ“Š Total rows available: {num_rows:,}\")\n",
    "    \n",
    "    # Pick random row\n",
    "    random_row = random.randint(0, num_rows - 1)\n",
    "    print(f\"ğŸ² Selected random row index: {random_row}\")\n",
    "    \n",
    "    # Generate paragraph\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Generating paragraph...\")\n",
    "    print(f\"{'â”€' * 80}\\n\")\n",
    "    \n",
    "    structure_id, coordinates, paragraph = generate_paragraph_for_row(random_row, pairs)\n",
    "    \n",
    "    # Save to log\n",
    "    log_file = output_folder / f\"test_paragraph_{structure_id}.txt\"\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STRUCTURE_ID: {structure_id}\\n\")\n",
    "        f.write(f\"COORDINATES: {coordinates}\\n\")\n",
    "        f.write(f\"ROW_INDEX: {random_row}\\n\")\n",
    "        f.write(f\"\\n{'=' * 80}\\n\")\n",
    "        f.write(f\"PARAGRAPH:\\n\")\n",
    "        f.write(f\"{'=' * 80}\\n\\n\")\n",
    "        f.write(paragraph)\n",
    "    \n",
    "    print(f\"âœ… Saved to: {log_file}\")\n",
    "    print(f\"\\nğŸ“ Paragraph length: {len(paragraph):,} characters\")\n",
    "    print(f\"ğŸ“ Word count: {len(paragraph.split()):,} words\")\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Preview (first 500 characters):\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(paragraph[:500] + \"...\\n\")\n",
    "\n",
    "def generate_all_paragraphs(schema_folder: str, data_folder: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Generate paragraphs for all bridges and save to CSV.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    print_section(\"GENERATING PARAGRAPHS FOR ALL BRIDGES\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Validate row counts match\n",
    "    print(\"\\nğŸ“Š Validating row counts...\")\n",
    "    row_counts = [len(df) for _, df, _ in pairs]\n",
    "    if len(set(row_counts)) > 1:\n",
    "        print(\"âŒ ERROR: Row count mismatch across CSVs!\")\n",
    "        for schema, df, name in pairs:\n",
    "            print(f\"   {name}: {len(df):,} rows\")\n",
    "        raise ValueError(\"Row count mismatch\")\n",
    "    \n",
    "    num_rows = row_counts[0]\n",
    "    print(f\"âœ… All CSVs have {num_rows:,} rows\")\n",
    "    \n",
    "    # Generate paragraphs\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Generating paragraphs...\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for row_idx in range(num_rows):\n",
    "        if row_idx % 100 == 0:\n",
    "            print(f\"Progress: {row_idx:,} / {num_rows:,} ({100 * row_idx / num_rows:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            structure_id, coordinates, paragraph = generate_paragraph_for_row(row_idx, pairs)\n",
    "            results.append({\n",
    "                'STRUCTURE_ID': structure_id,\n",
    "                'COORDINATES': coordinates,\n",
    "                'PARAGRAPH': paragraph\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error at row {row_idx}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"\\nğŸ’¾ Saving to {output_path}...\")\n",
    "    df_output = pd.DataFrame(results)\n",
    "    df_output.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print_section(\"COMPLETE\")\n",
    "    print(f\"âœ… Generated {len(results):,} paragraphs\")\n",
    "\n",
    "    print(f\"ğŸ’¾ Saved to: {output_path}\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")\n",
    "\n",
    "    print(f\"\\nğŸ“ Statistics:\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")\n",
    "\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Max paragraph length: {df_output['PARAGRAPH'].str.len().max():,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35778998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    TEST MODE - Random Paragraph Generation                     \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Total rows available: 4,914\n",
      "ğŸ² Selected random row index: 456\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Generating paragraph...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… Saved to: logs\\test_paragraph_0005142A.txt\n",
      "\n",
      "ğŸ“ Paragraph length: 21,808 characters\n",
      "ğŸ“ Word count: 2,923 words\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Preview (first 500 characters):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Design Peak Ground Acceleration: High seismic demand bridges with PGA values between 0.4 and 0.6 must incorporate advanced engineering techniques to withstand significant seismic forces and prevent structural damage. Short-Period Spectral Acceleration: High seismic demand on bridges, indicated by SS values between 1.0 and 1.5, necessitates advanced seismic design strategies, including the use of ductile materials and energy dissipation systems to maintain structural integrity. Spectral Accelerat...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test mode - generate one random paragraph\n",
    "test_random_paragraph(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad6be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                     GENERATING PARAGRAPHS FOR ALL BRIDGES                      \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Validating row counts...\n",
      "âœ… All CSVs have 4,914 rows\n",
      "\n",
      "================================================================================\n",
      "Generating paragraphs...\n",
      "================================================================================\n",
      "\n",
      "Progress: 0 / 4,914 (0.0%)\n",
      "Progress: 100 / 4,914 (2.0%)\n",
      "Progress: 200 / 4,914 (4.1%)\n",
      "Progress: 300 / 4,914 (6.1%)\n",
      "Progress: 400 / 4,914 (8.1%)\n",
      "Progress: 500 / 4,914 (10.2%)\n",
      "Progress: 600 / 4,914 (12.2%)\n",
      "Progress: 700 / 4,914 (14.2%)\n",
      "Progress: 800 / 4,914 (16.3%)\n",
      "Progress: 900 / 4,914 (18.3%)\n",
      "Progress: 1,000 / 4,914 (20.4%)\n",
      "Progress: 1,100 / 4,914 (22.4%)\n",
      "Progress: 1,200 / 4,914 (24.4%)\n",
      "Progress: 1,300 / 4,914 (26.5%)\n",
      "Progress: 1,400 / 4,914 (28.5%)\n",
      "Progress: 1,500 / 4,914 (30.5%)\n",
      "Progress: 1,600 / 4,914 (32.6%)\n",
      "Progress: 1,700 / 4,914 (34.6%)\n",
      "Progress: 1,800 / 4,914 (36.6%)\n",
      "Progress: 1,900 / 4,914 (38.7%)\n",
      "Progress: 2,000 / 4,914 (40.7%)\n",
      "Progress: 2,100 / 4,914 (42.7%)\n",
      "Progress: 2,200 / 4,914 (44.8%)\n",
      "Progress: 2,300 / 4,914 (46.8%)\n",
      "Progress: 2,400 / 4,914 (48.8%)\n",
      "Progress: 2,500 / 4,914 (50.9%)\n",
      "Progress: 2,600 / 4,914 (52.9%)\n",
      "Progress: 2,700 / 4,914 (54.9%)\n",
      "Progress: 2,800 / 4,914 (57.0%)\n",
      "Progress: 2,900 / 4,914 (59.0%)\n",
      "Progress: 3,000 / 4,914 (61.1%)\n",
      "Progress: 3,100 / 4,914 (63.1%)\n",
      "Progress: 3,200 / 4,914 (65.1%)\n",
      "Progress: 3,300 / 4,914 (67.2%)\n",
      "Progress: 3,400 / 4,914 (69.2%)\n",
      "Progress: 3,500 / 4,914 (71.2%)\n",
      "Progress: 3,600 / 4,914 (73.3%)\n",
      "Progress: 3,700 / 4,914 (75.3%)\n",
      "Progress: 3,800 / 4,914 (77.3%)\n",
      "Progress: 3,900 / 4,914 (79.4%)\n",
      "Progress: 4,000 / 4,914 (81.4%)\n",
      "Progress: 4,100 / 4,914 (83.4%)\n",
      "Progress: 4,200 / 4,914 (85.5%)\n",
      "Progress: 4,300 / 4,914 (87.5%)\n",
      "Progress: 4,400 / 4,914 (89.5%)\n",
      "Progress: 4,500 / 4,914 (91.6%)\n",
      "Progress: 4,600 / 4,914 (93.6%)\n",
      "Progress: 4,700 / 4,914 (95.6%)\n",
      "Progress: 4,800 / 4,914 (97.7%)\n",
      "Progress: 4,900 / 4,914 (99.7%)\n",
      "\n",
      "ğŸ’¾ Saving to C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_long.csv...\n",
      "\n",
      "================================================================================\n",
      "                                    COMPLETE                                    \n",
      "================================================================================\n",
      "\n",
      "âœ… Generated 4,914 paragraphs\n",
      "ğŸ’¾ Saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_long.csv\n",
      "   Min paragraph length: 19,167 characters\n",
      "\n",
      "ğŸ“ Statistics:\n",
      "   Min paragraph length: 19,167 characters\n",
      "   Average paragraph length: 21794 characters\n",
      "   Average paragraph length: 21794 characters\n",
      "   Max paragraph length: 23,344 characters\n"
     ]
    }
   ],
   "source": [
    "# Full generation\n",
    "generate_all_paragraphs(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\",\n",
    "    output_file=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_long.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464a1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_label(value: float, edges: List, labels: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find which bin the value falls into and return corresponding label.\n",
    "    Uses labels instead of sentences for snappier output.\n",
    "    \"\"\"\n",
    "    # Convert \"inf\" strings to float('inf')\n",
    "    edges_numeric = []\n",
    "    for e in edges:\n",
    "        if isinstance(e, str) and e.lower() in ['inf', '-inf']:\n",
    "            edges_numeric.append(float(e))\n",
    "        else:\n",
    "            edges_numeric.append(float(e))\n",
    "    \n",
    "    # Find bin index\n",
    "    bin_index = 0\n",
    "    for i in range(len(edges_numeric) - 1):\n",
    "        if value >= edges_numeric[i] and value < edges_numeric[i + 1]:\n",
    "            bin_index = i\n",
    "            break\n",
    "        elif i == len(edges_numeric) - 2:  # Last bin (to infinity)\n",
    "            bin_index = i\n",
    "    \n",
    "    return labels[bin_index]\n",
    "\n",
    "def generate_sentence_snappy(column_name: str, value: Any, schema_field: Dict, exclude_numerical: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Generate a snappier sentence using labels instead of full sentences.\n",
    "    Returns: \"{title_snake_case}: {label}\" in lowercase\n",
    "    \n",
    "    Args:\n",
    "        exclude_numerical: If True, skip numerical and numerical_coded fields\n",
    "    \"\"\"\n",
    "    # Skip null/missing values\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    title = schema_field.get('title', column_name)\n",
    "    # Convert title to snake_case\n",
    "    title_snake = title.replace(' ', '_')\n",
    "    \n",
    "    field_type = schema_field.get('type')\n",
    "    \n",
    "    # Skip numerical types if exclude_numerical is True\n",
    "    if exclude_numerical and field_type in ['numerical', 'numerical_coded']:\n",
    "        return None\n",
    "    \n",
    "    if field_type == 'nominal':\n",
    "        # Match value to code_map\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        if sentence:\n",
    "            return f\"{title_snake}: {ensure_period(sentence)}\".lower()\n",
    "    \n",
    "    elif field_type == 'nl':\n",
    "        # Use raw value as sentence (unchanged)\n",
    "        return f\"{title_snake}: {ensure_period(str(value))}\".lower()\n",
    "    \n",
    "    elif field_type == 'numerical':\n",
    "        # Use labels instead of sentences\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        labels = semantic_bins.get('labels', [])\n",
    "        \n",
    "        if edges and labels:\n",
    "            try:\n",
    "                label = get_numerical_label(float(value), edges, labels)\n",
    "                return f\"{title_snake}: {ensure_period(label)}\".lower()\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    elif field_type == 'numerical_coded':\n",
    "        # Check code_map first - if value is in code_map, skip it\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        if str(value) in code_map or (isinstance(value, (int, float)) and str(int(value)) in code_map):\n",
    "            return None  # Skip coded values\n",
    "        \n",
    "        # Otherwise use labels like numerical\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        labels = semantic_bins.get('labels', [])\n",
    "        \n",
    "        if edges and labels:\n",
    "            try:\n",
    "                label = get_numerical_label(float(value), edges, labels)\n",
    "                return f\"{title_snake}: {ensure_period(label)}\".lower()\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical_coded value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def generate_paragraph_snappy(row_index: int, schema_csv_pairs: List[Tuple[Dict, pd.DataFrame, str]], exclude_numerical: bool = False) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generate a snappy paragraph for a single row across all CSVs.\n",
    "    \n",
    "    Args:\n",
    "        exclude_numerical: If True, skip numerical and numerical_coded fields\n",
    "    \n",
    "    Returns: (structure_id, coordinates, paragraph)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    structure_id = None\n",
    "    coordinates = None\n",
    "    \n",
    "    for schema, df, source_name in schema_csv_pairs:\n",
    "        row = df.iloc[row_index]\n",
    "        \n",
    "        # Extract structure_id and coordinates from first pair\n",
    "        if structure_id is None:\n",
    "            structure_id = row.iloc[0] if 'STRUCTURE_ID' not in df.columns else row['STRUCTURE_ID']\n",
    "        if coordinates is None:\n",
    "            coordinates = row.iloc[1] if 'COORDINATES' not in df.columns else row['COORDINATES']\n",
    "        \n",
    "        # Process each column (skip STRUCTURE_ID and COORDINATES)\n",
    "        for column_name, schema_field in schema.items():\n",
    "            if column_name in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "                continue\n",
    "            \n",
    "            # Skip reference and redundant types\n",
    "            field_type = schema_field.get('type')\n",
    "            if field_type in ['reference', 'numerical_redundant', 'nominal_redundant', 'numerical_coded_redundant']:\n",
    "                continue\n",
    "            \n",
    "            if column_name not in df.columns:\n",
    "                raise ValueError(f\"Column mismatch: {column_name}\")\n",
    "            \n",
    "            value = row[column_name]\n",
    "            sentence = generate_sentence_snappy(column_name, value, schema_field, exclude_numerical)\n",
    "            \n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    # Combine all sentences into one paragraph\n",
    "    paragraph = \" \".join(sentences)\n",
    "    \n",
    "    return str(structure_id), str(coordinates), paragraph\n",
    "\n",
    "def generate_all_paragraphs_snappy(schema_folder: str, data_folder: str, output_file: str, exclude_numerical: bool = False):\n",
    "    \"\"\"\n",
    "    Generate snappy paragraphs for all bridges and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        exclude_numerical: If True, skip numerical and numerical_coded fields\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    print_section(\"GENERATING SNAPPY PARAGRAPHS FOR ALL BRIDGES\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Validate row counts match\n",
    "    print(\"\\nğŸ“Š Validating row counts...\")\n",
    "    row_counts = [len(df) for _, df, _ in pairs]\n",
    "    if len(set(row_counts)) > 1:\n",
    "        print(\"âŒ ERROR: Row count mismatch across CSVs!\")\n",
    "        for schema, df, name in pairs:\n",
    "            print(f\"   {name}: {len(df):,} rows\")\n",
    "        raise ValueError(\"Row count mismatch\")\n",
    "    \n",
    "    num_rows = row_counts[0]\n",
    "    print(f\"âœ… All CSVs have {num_rows:,} rows\")\n",
    "    \n",
    "    # Generate paragraphs\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Generating snappy paragraphs...\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for row_idx in range(num_rows):\n",
    "        if row_idx % 100 == 0:\n",
    "            print(f\"Progress: {row_idx:,} / {num_rows:,} ({100 * row_idx / num_rows:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            structure_id, coordinates, paragraph = generate_paragraph_snappy(row_idx, pairs, exclude_numerical)\n",
    "            results.append({\n",
    "                'STRUCTURE_ID': structure_id,\n",
    "                'COORDINATES': coordinates,\n",
    "                'PARAGRAPH': paragraph\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error at row {row_idx}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"\\nğŸ’¾ Saving to {output_path}...\")\n",
    "    df_output = pd.DataFrame(results)\n",
    "    df_output.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print_section(\"COMPLETE\")\n",
    "    print(f\"âœ… Generated {len(results):,} paragraphs\")\n",
    "    print(f\"ğŸ’¾ Saved to: {output_path}\")\n",
    "    print(f\"\\nğŸ“ Statistics:\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Max paragraph length: {df_output['PARAGRAPH'].str.len().max():,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b93cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                  GENERATING SNAPPY PARAGRAPHS FOR ALL BRIDGES                  \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Validating row counts...\n",
      "âœ… All CSVs have 4,914 rows\n",
      "\n",
      "================================================================================\n",
      "Generating snappy paragraphs...\n",
      "================================================================================\n",
      "\n",
      "Progress: 0 / 4,914 (0.0%)\n",
      "Progress: 100 / 4,914 (2.0%)\n",
      "Progress: 200 / 4,914 (4.1%)\n",
      "Progress: 300 / 4,914 (6.1%)\n",
      "Progress: 400 / 4,914 (8.1%)\n",
      "Progress: 500 / 4,914 (10.2%)\n",
      "Progress: 600 / 4,914 (12.2%)\n",
      "Progress: 700 / 4,914 (14.2%)\n",
      "Progress: 800 / 4,914 (16.3%)\n",
      "Progress: 900 / 4,914 (18.3%)\n",
      "Progress: 1,000 / 4,914 (20.4%)\n",
      "Progress: 1,100 / 4,914 (22.4%)\n",
      "Progress: 1,200 / 4,914 (24.4%)\n",
      "Progress: 1,300 / 4,914 (26.5%)\n",
      "Progress: 1,400 / 4,914 (28.5%)\n",
      "Progress: 1,500 / 4,914 (30.5%)\n",
      "Progress: 1,600 / 4,914 (32.6%)\n",
      "Progress: 1,700 / 4,914 (34.6%)\n",
      "Progress: 1,800 / 4,914 (36.6%)\n",
      "Progress: 1,900 / 4,914 (38.7%)\n",
      "Progress: 2,000 / 4,914 (40.7%)\n",
      "Progress: 2,100 / 4,914 (42.7%)\n",
      "Progress: 2,200 / 4,914 (44.8%)\n",
      "Progress: 2,300 / 4,914 (46.8%)\n",
      "Progress: 2,400 / 4,914 (48.8%)\n",
      "Progress: 2,500 / 4,914 (50.9%)\n",
      "Progress: 2,600 / 4,914 (52.9%)\n",
      "Progress: 2,700 / 4,914 (54.9%)\n",
      "Progress: 2,800 / 4,914 (57.0%)\n",
      "Progress: 2,900 / 4,914 (59.0%)\n",
      "Progress: 3,000 / 4,914 (61.1%)\n",
      "Progress: 3,100 / 4,914 (63.1%)\n",
      "Progress: 3,200 / 4,914 (65.1%)\n",
      "Progress: 3,300 / 4,914 (67.2%)\n",
      "Progress: 3,400 / 4,914 (69.2%)\n",
      "Progress: 3,500 / 4,914 (71.2%)\n",
      "Progress: 3,600 / 4,914 (73.3%)\n",
      "Progress: 3,700 / 4,914 (75.3%)\n",
      "Progress: 3,800 / 4,914 (77.3%)\n",
      "Progress: 3,900 / 4,914 (79.4%)\n",
      "Progress: 4,000 / 4,914 (81.4%)\n",
      "Progress: 4,100 / 4,914 (83.4%)\n",
      "Progress: 4,200 / 4,914 (85.5%)\n",
      "Progress: 4,300 / 4,914 (87.5%)\n",
      "Progress: 4,400 / 4,914 (89.5%)\n",
      "Progress: 4,500 / 4,914 (91.6%)\n",
      "Progress: 4,600 / 4,914 (93.6%)\n",
      "Progress: 4,700 / 4,914 (95.6%)\n",
      "Progress: 4,800 / 4,914 (97.7%)\n",
      "Progress: 4,900 / 4,914 (99.7%)\n",
      "\n",
      "ğŸ’¾ Saving to C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_short.csv...\n",
      "\n",
      "================================================================================\n",
      "                                    COMPLETE                                    \n",
      "================================================================================\n",
      "\n",
      "âœ… Generated 4,914 paragraphs\n",
      "ğŸ’¾ Saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_short.csv\n",
      "\n",
      "ğŸ“ Statistics:\n",
      "   Min paragraph length: 3,336 characters\n",
      "   Average paragraph length: 4181 characters\n",
      "   Max paragraph length: 5,162 characters\n"
     ]
    }
   ],
   "source": [
    "generate_all_paragraphs_snappy(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\",\n",
    "    output_file=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_short.csv\",\n",
    "    exclude_numerical= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852f523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual edits\n",
    "\"\"\"\n",
    "\"NFHL_FLD_ZONE\": {\n",
    "    \"type\": \"nominal\",\n",
    "    \"description\": \"The flood hazard zone classification describes the level and type of flood risk assigned to the location based on FEMA floodplain mapping.\",\n",
    "    \"code_map\": {\n",
    "      \"A\": \"high risk no elevation data\",\n",
    "      \"AE\": \"high risk with elevation data\",\n",
    "      \"AO\": \"shallow flooding sheet flow\",\n",
    "      \"AH\": \"shallow flooding with elevation\",\n",
    "      \"V\": \"coastal high velocity zone\",\n",
    "      \"VE\": \"coastal velocity with elevation\",\n",
    "      \"X\": \"minimal flood risk\",\n",
    "      \"X (0.2%)\": \"moderate risk low probability\",\n",
    "      \"D\": \"undetermined flood hazard\"\n",
    "    },\n",
    "    \"title\": \"Flood Hazard Zone\"\n",
    "  }\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "  \"NFHL_SFHA\": {\n",
    "    \"type\": \"nominal\",\n",
    "    \"description\": \"The indicator of Special Flood Hazard Area status describes whether the location is subject to a 1-percent-annual-chance (100-year) flood hazard.\",\n",
    "    \"code_map\": {\n",
    "      \"T\": \"within special flood hazard area\",\n",
    "      \"F\": \"outside special flood hazard area\"\n",
    "    },\n",
    "    \"title\": \"Flood Hazard Status\"\n",
    "  },\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "  \"SDCS\": {\n",
    "    \"type\": \"nominal\",\n",
    "    \"description\": \"The seismic design category that classifies overall seismic risk at the site and governs required detailing, analysis rigor, and design provisions.\",\n",
    "    \"code_map\": {\n",
    "      \"A\": \"very low seismic hazard\",\n",
    "      \"B\": \"low seismic hazard\",\n",
    "      \"C\": \"moderate seismic hazard\",\n",
    "      \"D\": \"high seismic hazard\",\n",
    "      \"E\": \"very high seismic hazard\",\n",
    "      \"F\": \"extreme site specific hazard\"\n",
    "    },\n",
    "    \"title\": \"Seismic Design Category\"\n",
    "  },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73004b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                 TEST MODE - Random Snappy Paragraph Generation                 \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Total rows available: 4,914\n",
      "ğŸ² Selected random row index: 4684\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Generating snappy paragraph...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… Saved to: logs\\test_paragraph_snappy_88866000.txt\n",
      "\n",
      "ğŸ“ Paragraph length: 4,438 characters\n",
      "ğŸ“ Word count: 402 words\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Preview (first 500 characters):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "design_peak_ground_acceleration: very high seismic demand. spectral_acceleration_1.0s: very high seismic demand. design_spectral_acceleration: very high seismic demand. seismic_design_category: high seismic hazard. ground_motion_factor: minimal seismic impact. geologic_unit_name: mesozoic-tertiary marine rocks, undivided. stratigraphic_unit: hoh formation. rock_material_type: major:{graywacke,slate,argillite}, minor:{arkose,siltstone,conglomerate,phyllite}, incidental:{basalt, gabbro, coal}. min...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_random_paragraph_snappy(schema_folder: str, data_folder: str, output_folder: str = \"logs\", exclude_numerical: bool = False):\n",
    "    \"\"\"\n",
    "    Generate a snappy paragraph for a random bridge and save to log file.\n",
    "    \n",
    "    Args:\n",
    "        exclude_numerical: If True, skip numerical and numerical_coded fields\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    print_section(\"TEST MODE - Random Snappy Paragraph Generation\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Get row count from first dataframe\n",
    "    num_rows = len(pairs[0][1])\n",
    "    print(f\"\\nğŸ“Š Total rows available: {num_rows:,}\")\n",
    "    \n",
    "    # Pick random row\n",
    "    random_row = random.randint(0, num_rows - 1)\n",
    "    print(f\"ğŸ² Selected random row index: {random_row}\")\n",
    "    \n",
    "    # Generate paragraph\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Generating snappy paragraph...\")\n",
    "    print(f\"{'â”€' * 80}\\n\")\n",
    "    \n",
    "    structure_id, coordinates, paragraph = generate_paragraph_snappy(random_row, pairs, exclude_numerical)\n",
    "    \n",
    "    # Save to log\n",
    "    log_file = output_folder / f\"test_paragraph_snappy_{structure_id}.txt\"\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STRUCTURE_ID: {structure_id}\\n\")\n",
    "        f.write(f\"COORDINATES: {coordinates}\\n\")\n",
    "        f.write(f\"ROW_INDEX: {random_row}\\n\")\n",
    "        f.write(f\"\\n{'=' * 80}\\n\")\n",
    "        f.write(f\"SNAPPY PARAGRAPH:\\n\")\n",
    "        f.write(f\"{'=' * 80}\\n\\n\")\n",
    "        f.write(paragraph)\n",
    "    \n",
    "    print(f\"âœ… Saved to: {log_file}\")\n",
    "    print(f\"\\nğŸ“ Paragraph length: {len(paragraph):,} characters\")\n",
    "    print(f\"ğŸ“ Word count: {len(paragraph.split()):,} words\")\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Preview (first 500 characters):\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(paragraph[:500] + \"...\\n\")\n",
    "\n",
    "# Test mode - generate one random snappy paragraph\n",
    "test_random_paragraph_snappy(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b88ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
