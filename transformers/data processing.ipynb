{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1d0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csvs_by_id_and_coords(folder_path, output_path):\n",
    "    \"\"\"\n",
    "    Merges all CSVs in a folder on STRUCTURE_ID and COORDINATES columns (first two columns),\n",
    "    and saves the result as a master CSV.\n",
    "    \"\"\"\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        dfs.append(df)\n",
    "    # Merge all on STRUCTURE_ID and COORDINATES (inner join)\n",
    "    from functools import reduce\n",
    "    master_df = reduce(lambda left, right: pd.merge(left, right, on=[left.columns[0], left.columns[1]], how='inner'), dfs)\n",
    "    master_df.to_csv(output_path, index=False)\n",
    "    return master_df\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\wongb\\\\Bridge-ML\\\\Bridge-ML-LLM-Embedding-Architecture\\\\part1 clean\\\\enriched data'\n",
    "output_path = 'C:\\\\Users\\\\wongb\\\\Bridge-ML\\\\Bridge-ML-LLM-Embedding-Architecture\\\\part1 clean\\\\master data\\\\master.csv'\n",
    "merged = merge_csvs_by_id_and_coords(folder_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2faaff43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STRUCTURE_ID</th>\n",
       "      <th>COORDINATES</th>\n",
       "      <th>PGA</th>\n",
       "      <th>SS</th>\n",
       "      <th>S1</th>\n",
       "      <th>SMS</th>\n",
       "      <th>SDS</th>\n",
       "      <th>SDCS</th>\n",
       "      <th>PGAM</th>\n",
       "      <th>FPGA</th>\n",
       "      <th>...</th>\n",
       "      <th>PGA_2475</th>\n",
       "      <th>PGA_10000</th>\n",
       "      <th>SA02_475</th>\n",
       "      <th>SA02_975</th>\n",
       "      <th>SA02_2475</th>\n",
       "      <th>SA02_10000</th>\n",
       "      <th>SA10_475</th>\n",
       "      <th>SA10_975</th>\n",
       "      <th>SA10_2475</th>\n",
       "      <th>SA10_10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1W</td>\n",
       "      <td>(48.29745556, -122.6078139)</td>\n",
       "      <td>0.591</td>\n",
       "      <td>1.359</td>\n",
       "      <td>0.487</td>\n",
       "      <td>1.359</td>\n",
       "      <td>0.906</td>\n",
       "      <td>D</td>\n",
       "      <td>0.650</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617783</td>\n",
       "      <td>1.010145</td>\n",
       "      <td>0.717051</td>\n",
       "      <td>1.028333</td>\n",
       "      <td>1.547281</td>\n",
       "      <td>2.592843</td>\n",
       "      <td>0.207841</td>\n",
       "      <td>0.300969</td>\n",
       "      <td>0.457674</td>\n",
       "      <td>0.774146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>(47.769275, -122.707925)</td>\n",
       "      <td>0.503</td>\n",
       "      <td>1.368</td>\n",
       "      <td>0.487</td>\n",
       "      <td>1.368</td>\n",
       "      <td>0.912</td>\n",
       "      <td>D</td>\n",
       "      <td>0.553</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687606</td>\n",
       "      <td>1.092013</td>\n",
       "      <td>0.832990</td>\n",
       "      <td>1.169996</td>\n",
       "      <td>1.728167</td>\n",
       "      <td>2.864262</td>\n",
       "      <td>0.234359</td>\n",
       "      <td>0.334211</td>\n",
       "      <td>0.494918</td>\n",
       "      <td>0.801621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>(47.56759167, -122.5517028)</td>\n",
       "      <td>0.689</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.609</td>\n",
       "      <td>1.073</td>\n",
       "      <td>D</td>\n",
       "      <td>0.757</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815340</td>\n",
       "      <td>1.327535</td>\n",
       "      <td>0.940753</td>\n",
       "      <td>1.356070</td>\n",
       "      <td>2.056026</td>\n",
       "      <td>3.450107</td>\n",
       "      <td>0.313007</td>\n",
       "      <td>0.463082</td>\n",
       "      <td>0.720413</td>\n",
       "      <td>1.230125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000000</td>\n",
       "      <td>(47.25279444, -124.178075)</td>\n",
       "      <td>0.771</td>\n",
       "      <td>1.568</td>\n",
       "      <td>0.745</td>\n",
       "      <td>1.568</td>\n",
       "      <td>1.045</td>\n",
       "      <td>D</td>\n",
       "      <td>0.848</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940662</td>\n",
       "      <td>1.736888</td>\n",
       "      <td>0.715402</td>\n",
       "      <td>1.259048</td>\n",
       "      <td>2.227996</td>\n",
       "      <td>4.219385</td>\n",
       "      <td>0.203650</td>\n",
       "      <td>0.380466</td>\n",
       "      <td>0.678461</td>\n",
       "      <td>1.240071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>(47.98571667, -122.2271222)</td>\n",
       "      <td>0.542</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.840</td>\n",
       "      <td>D</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536230</td>\n",
       "      <td>0.874637</td>\n",
       "      <td>0.625251</td>\n",
       "      <td>0.891298</td>\n",
       "      <td>1.343215</td>\n",
       "      <td>2.275001</td>\n",
       "      <td>0.204856</td>\n",
       "      <td>0.293965</td>\n",
       "      <td>0.442995</td>\n",
       "      <td>0.744733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  STRUCTURE_ID                  COORDINATES    PGA     SS     S1    SMS  \\\n",
       "0           1W  (48.29745556, -122.6078139)  0.591  1.359  0.487  1.359   \n",
       "1                  (47.769275, -122.707925)  0.503  1.368  0.487  1.368   \n",
       "2               (47.56759167, -122.5517028)  0.689  1.609  0.560  1.609   \n",
       "3     00000000   (47.25279444, -124.178075)  0.771  1.568  0.745  1.568   \n",
       "4               (47.98571667, -122.2271222)  0.542  1.260  0.447  1.260   \n",
       "\n",
       "     SDS SDCS   PGAM  FPGA  ...  PGA_2475 PGA_10000  SA02_475  SA02_975  \\\n",
       "0  0.906    D  0.650   1.1  ...  0.617783  1.010145  0.717051  1.028333   \n",
       "1  0.912    D  0.553   1.1  ...  0.687606  1.092013  0.832990  1.169996   \n",
       "2  1.073    D  0.757   1.1  ...  0.815340  1.327535  0.940753  1.356070   \n",
       "3  1.045    D  0.848   1.1  ...  0.940662  1.736888  0.715402  1.259048   \n",
       "4  0.840    D  0.596   1.1  ...  0.536230  0.874637  0.625251  0.891298   \n",
       "\n",
       "   SA02_2475 SA02_10000  SA10_475  SA10_975  SA10_2475  SA10_10000  \n",
       "0   1.547281   2.592843  0.207841  0.300969   0.457674    0.774146  \n",
       "1   1.728167   2.864262  0.234359  0.334211   0.494918    0.801621  \n",
       "2   2.056026   3.450107  0.313007  0.463082   0.720413    1.230125  \n",
       "3   2.227996   4.219385  0.203650  0.380466   0.678461    1.240071  \n",
       "4   1.343215   2.275001  0.204856  0.293965   0.442995    0.744733  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def drop_all_null_columns(csv_path):\n",
    "    \"\"\"\n",
    "    Drops columns with 100% null values from the CSV and saves the cleaned DataFrame back to the same path.\n",
    "    Returns the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Find columns with 100% nulls\n",
    "    all_null_cols = df.columns[df.isnull().mean() == 1.0]\n",
    "    df_cleaned = df.drop(columns=all_null_cols)\n",
    "    df_cleaned.to_csv(csv_path, index=False)\n",
    "    return df_cleaned\n",
    "\n",
    "# Drop 100% null columns and overwrite CSV\n",
    "csv_path = 'C:\\\\Users\\\\wongb\\\\Bridge-ML\\\\Bridge-ML-LLM-Embedding-Architecture\\\\part1 clean\\\\master data\\\\master.csv'\n",
    "df_cleaned = drop_all_null_columns(csv_path)\n",
    "display(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def merge_schema_jsons(folder_path, output_path):\n",
    "    \"\"\"\n",
    "    Merges all key-value pairs from JSON files (schemas) in a folder into one master schema dictionary and saves as a JSON.\n",
    "    \"\"\"\n",
    "    master_schema = {}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:\n",
    "                schema = json.load(f)\n",
    "                master_schema.update(schema)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(master_schema, f, indent=2)\n",
    "    return master_schema\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\wongb\\\\Bridge-ML\\\\Bridge-ML-LLM-Embedding-Architecture\\\\part1 clean\\\\schemas'\n",
    "output_path = 'C:\\\\Users\\\\wongb\\\\Bridge-ML\\\\Bridge-ML-LLM-Embedding-Architecture\\\\part1 clean\\\\master data\\\\master_schema.json'\n",
    "merged_schema = merge_schema_jsons(folder_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbcfd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def drop_redundant_and_reference_columns(master_csv_path, master_schema_path, cleaned_csv_path, cleaned_schema_path):\n",
    "    \"\"\"\n",
    "    Drops columns from master dataset and keys from schema whose 'type' attribute contains 'redundant' or is 'reference',\n",
    "    except for columns named 'STRUCTURE_ID' and 'COORDINATES'.\n",
    "    Saves cleaned dataset and schema to specified paths.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(master_csv_path)\n",
    "    with open(master_schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    protected = {'STRUCTURE_ID', 'COORDINATES'}\n",
    "    # Find columns to drop\n",
    "    redundant_or_reference_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in protected and (\n",
    "            ('type' in schema.get(col, {}) and (\n",
    "                'redundant' in str(schema[col]['type']).lower() or\n",
    "                str(schema[col]['type']).lower() == 'reference'\n",
    "            ))\n",
    "        )\n",
    "    ]\n",
    "    # Drop from DataFrame\n",
    "    df_cleaned = df.drop(columns=redundant_or_reference_cols)\n",
    "    df_cleaned.to_csv(cleaned_csv_path, index=False)\n",
    "    # Drop from schema\n",
    "    cleaned_schema = {k: v for k, v in schema.items() if k not in redundant_or_reference_cols}\n",
    "    with open(cleaned_schema_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_schema, f, indent=2)\n",
    "    return df_cleaned, cleaned_schema\n",
    "\n",
    "# Example usage:\n",
    "master_csv_path = r'C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\part1 clean\\master data\\master.csv'\n",
    "master_schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/master_schema.json'\n",
    "cleaned_csv_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master.csv'\n",
    "cleaned_schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "df_cleaned, cleaned_schema = drop_redundant_and_reference_columns(master_csv_path, master_schema_path, cleaned_csv_path, cleaned_schema_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13f15369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference: 2\n",
      "numerical: 27\n",
      "nominal: 57\n",
      "nl: 8\n",
      "numerical_coded: 8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def count_schema_types(schema_path):\n",
    "    \"\"\"\n",
    "    Lists all unique 'type' values in the schema and counts their occurrences.\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    type_list = [v.get('type', 'unknown') for v in schema.values()]\n",
    "    type_counts = Counter(type_list)\n",
    "    for t, count in type_counts.items():\n",
    "        print(f\"{t}: {count}\")\n",
    "    return type_counts\n",
    "\n",
    "# Example usage:\n",
    "schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "type_counts = count_schema_types(schema_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf2f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def build_nl_fact_sentences(master_csv_path, master_schema_path, output_path):\n",
    "    \"\"\"\n",
    "    For each row in the master dataset, builds a sentence from all columns whose schema type is 'nl',\n",
    "    using their title attribute. Saves a 3-column CSV (STRUCTURE_ID, COORDINATES, text) to output_path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(master_csv_path)\n",
    "    with open(master_schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    # Find nl columns and their titles\n",
    "    nl_cols = [(col, schema[col]['title']) for col in df.columns if 'type' in schema.get(col, {}) and schema[col]['type'] == 'nl' and 'title' in schema[col]]\n",
    "    # Build sentences\n",
    "    def make_sentence(row):\n",
    "        parts = []\n",
    "        for col, title in nl_cols:\n",
    "            val = row[col]\n",
    "            if pd.notnull(val) and str(val).strip():\n",
    "                parts.append(f\"{title}: {val}.\")\n",
    "        return ' '.join(parts)\n",
    "    result_df = pd.DataFrame({\n",
    "        'STRUCTURE_ID': df['STRUCTURE_ID'],\n",
    "        'COORDINATES': df['COORDINATES'],\n",
    "        'text': df.apply(make_sentence, axis=1)\n",
    "    })\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "master_csv_path = r'C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\part1 clean\\master data\\cleaned_master.csv'\n",
    "master_schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/bridge_nl_sentences.csv'\n",
    "bridge_nl_df = build_nl_fact_sentences(master_csv_path, master_schema_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f9fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence token length: 362\n",
      "Average number of tokens per sentence: 134.30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/bridge_nl_sentences.csv')\n",
    "\n",
    "# Initialize tokenizer (example: BERT)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Check max token length in the dataset\n",
    "all_token_lens = [len(tokenizer.tokenize(s)) for s in df['text']]\n",
    "max_token_len = max(all_token_lens)\n",
    "print(f\"Longest sentence token length: {max_token_len}\")\n",
    "\n",
    "# Tokenize sentences (set max_length to max_token_len or 160)\n",
    "encoded = tokenizer(\n",
    "    df['text'].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=160,  # You can set this to max_token_len if you want no truncation\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Calculate average number of tokens (before padding/truncation)\n",
    "avg_tokens = sum(all_token_lens) / len(df)\n",
    "print(f\"Average number of tokens per sentence: {avg_tokens:.2f}\")\n",
    "\n",
    "# Add tokenized tensor as a new column\n",
    "# Convert each tensor row to a list\n",
    "tokens_list = [row.tolist() for row in encoded['input_ids']]\n",
    "df['tokens'] = tokens_list\n",
    "\n",
    "# Save updated DataFrame\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/bridge_nl_sentences_with_tokens.csv'\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae896db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def recraft_nominal_schema_with_integer_codes(schema_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads a schema JSON, extracts STRUCTURE_ID, COORDINATES, and all nominal columns.\n",
    "    For each nominal column, creates a new code map with original keys mapping to integer values (1..n).\n",
    "    Saves the new schema to output_path.\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "\n",
    "    # Only keep STRUCTURE_ID, COORDINATES, and nominal columns\n",
    "    new_schema = {}\n",
    "    for col, col_info in schema.items():\n",
    "        if col in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "            new_schema[col] = col_info\n",
    "        elif col_info.get('type') == 'nominal' and 'code_map' in col_info:\n",
    "            code_map = col_info['code_map']\n",
    "            # Sort keys for reproducibility\n",
    "            sorted_keys = sorted(code_map.keys(), key=lambda x: str(x))\n",
    "            # Assign integer codes starting from 1\n",
    "            new_code_map = {k: i+1 for i, k in enumerate(sorted_keys)}\n",
    "            # Add new_code_map to column info\n",
    "            col_info['new_code_map'] = new_code_map\n",
    "            new_schema[col] = col_info\n",
    "    # Save new schema\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_schema, f, indent=2)\n",
    "    return new_schema\n",
    "\n",
    "# Example usage:\n",
    "schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/cat/nominal_integer_schema.json'\n",
    "recrafted_schema = recraft_nominal_schema_with_integer_codes(schema_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0b885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def apply_nominal_integer_encoding(schema_path, csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads a schema with new_code_map for nominal columns and applies integer encoding to the dataset.\n",
    "    Only STRUCTURE_ID, COORDINATES, and nominal columns are kept.\n",
    "    Handles both integer and string values in the dataset, and missing values.\n",
    "    The nominal columns are replaced using the new_code_map.\n",
    "    Saves the result to output_path.\n",
    "    \"\"\"\n",
    "    # Load schema\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Determine columns to keep\n",
    "    keep_cols = [col for col in schema.keys() if col in df.columns]\n",
    "    df = df[keep_cols]\n",
    "    # Apply integer encoding to nominal columns\n",
    "    for col, col_info in schema.items():\n",
    "        if col in df.columns and col_info.get('type') == 'nominal' and 'new_code_map' in col_info:\n",
    "            code_map = col_info['new_code_map']\n",
    "            # Ensure all keys in code_map are strings for matching\n",
    "            code_map_str = {str(k): v for k, v in code_map.items()}\n",
    "            # Convert values to string for mapping, handle missing values\n",
    "            df[col] = df[col].apply(lambda x: code_map_str.get(str(x), pd.NA) if pd.notnull(x) else pd.NA)\n",
    "    # Save the encoded DataFrame\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/cat/nominal_integer_schema.json'\n",
    "csv_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master.csv'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/cat/nominal_integer_encoded.csv'\n",
    "encoded_df = apply_nominal_integer_encoding(schema_path, csv_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7909b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def extract_numerical_columns(schema_path, csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Extracts STRUCTURE_ID, COORDINATES, and all columns with type containing 'numerical' or 'numerical_coded' from the dataset.\n",
    "    Saves the result to output_path.\n",
    "    \"\"\"\n",
    "    # Load schema\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    # Find relevant columns\n",
    "    cols = []\n",
    "    for col, info in schema.items():\n",
    "        t = str(info.get('type', '')).lower()\n",
    "        if col in ['STRUCTURE_ID', 'COORDINATES'] or 'numerical' in t:\n",
    "            cols.append(col)\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Keep only relevant columns that exist in the dataset\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    df_out = df[cols]\n",
    "    df_out.to_csv(output_path, index=False)\n",
    "    return df_out\n",
    "\n",
    "# Example usage:\n",
    "schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "csv_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master.csv'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/num/numerical_columns.csv'\n",
    "numerical_df = extract_numerical_columns(schema_path, csv_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94566bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def process_and_normalize_numerical(schema_path, csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Keeps STRUCTURE_ID, COORDINATES, and all numerical/numerical_coded columns.\n",
    "    For 'numerical' columns: fillna(-999), then normalize.\n",
    "    For 'numerical_coded' columns: handle special_zero and special_max, set those and nulls to -999, then normalize.\n",
    "    Saves processed DataFrame to output_path.\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    keep_cols = ['STRUCTURE_ID', 'COORDINATES']\n",
    "    for col, info in schema.items():\n",
    "        t = str(info.get('type', '')).lower()\n",
    "        if 'numerical' in t:\n",
    "            keep_cols.append(col)\n",
    "    keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "    df_out = df[keep_cols].copy()\n",
    "    for col, info in schema.items():\n",
    "        t = str(info.get('type', '')).lower()\n",
    "        if col not in df_out.columns or col in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "            continue\n",
    "        if t == 'numerical':\n",
    "            df_out[col] = pd.to_numeric(df_out[col], errors='coerce').fillna(-999)\n",
    "            # Convert to float BEFORE normalization\n",
    "            df_out[col] = df_out[col].astype(float)\n",
    "            # Normalize (ignore -999 for mean/std)\n",
    "            mask = df_out[col] != -999\n",
    "            if mask.any():\n",
    "                mean = df_out.loc[mask, col].mean()\n",
    "                std = df_out.loc[mask, col].std()\n",
    "                if std == 0: std = 1\n",
    "                df_out.loc[mask, col] = (df_out.loc[mask, col] - mean) / std\n",
    "        elif t == 'numerical_coded':\n",
    "            col_vals = pd.to_numeric(df_out[col], errors='coerce')\n",
    "            # Handle special_zero\n",
    "            if info.get('special_zero', False):\n",
    "                col_vals = col_vals.mask(col_vals == 0, -999)\n",
    "            # Handle special_max\n",
    "            if info.get('special_max', False):\n",
    "                # Set any value ending in 99 (e.g., 99, 99.9, 999, 999.9, etc.) to -999\n",
    "                def is_special_max(x):\n",
    "                    try:\n",
    "                        s = str(int(float(x)))\n",
    "                        return s[-2:] == '99'\n",
    "                    except:\n",
    "                        return False\n",
    "                col_vals = col_vals.mask(col_vals.apply(is_special_max), -999)\n",
    "            # Set nulls to -999\n",
    "            col_vals = col_vals.fillna(-999).astype(float)  # Convert to float here\n",
    "            # Normalize (ignore -999 for mean/std)\n",
    "            mask = col_vals != -999\n",
    "            if mask.any():\n",
    "                mean = col_vals[mask].mean()\n",
    "                std = col_vals[mask].std()\n",
    "                if std == 0: std = 1\n",
    "                col_vals[mask] = (col_vals[mask] - mean) / std\n",
    "            df_out[col] = col_vals\n",
    "    df_out.to_csv(output_path, index=False)\n",
    "    return df_out\n",
    "\n",
    "# Example usage:\n",
    "schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "csv_path = 'C:\\\\Users\\\\wongb\\\\Bridge-ML\\\\Bridge-ML-LLM-Embedding-Architecture\\\\part1 clean\\\\master data\\\\segments\\\\num\\\\numerical_columns.csv'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/num/numerical_processed.csv'\n",
    "processed_df = process_and_normalize_numerical(schema_path, csv_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ce37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_categorical_nans(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads a CSV, sets all NaN values in categorical columns (excluding STRUCTURE_ID and COORDINATES) to -999, and saves the result.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Exclude STRUCTURE_ID and COORDINATES\n",
    "    cat_cols = [col for col in df.columns if col not in ['STRUCTURE_ID', 'COORDINATES']]\n",
    "    # Set NaNs to -999 in categorical columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].where(df[col].notna(), -999)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/cat/nominal_integer_encoded.csv'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/cat/nominal_integer_encoded_nans.csv'\n",
    "processed_cat_df = process_categorical_nans(csv_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a7ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def join_csvs_on_id_and_coords(csv_paths, output_path):\n",
    "    \"\"\"\n",
    "    Takes a list of CSV file paths, joins them on STRUCTURE_ID and COORDINATES, and saves the merged DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = [pd.read_csv(path) for path in csv_paths]\n",
    "    from functools import reduce\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on=['STRUCTURE_ID', 'COORDINATES'], how='inner'), dfs)\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    return merged_df\n",
    "\n",
    "# Example usage:\n",
    "csv_paths = [\n",
    "    'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/cat/nominal_integer_encoded_nans.csv',\n",
    "    'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/num/numerical_processed.csv',\n",
    "    'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/txt/bridge_nl_sentences_with_tokens.csv'\n",
    "    # Add more paths as needed\n",
    "    ]\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/merged_all.csv'\n",
    "merged_all_df = join_csvs_on_id_and_coords(csv_paths, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c59736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def finalize_merged_dataset(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads the merged dataset, removes the 'text' column, combines STRUCTURE_ID and COORDINATES into a unified identifier tuple as the first column,\n",
    "    handles null STRUCTURE_IDs by setting them to -999, and saves the result.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Remove 'text' column if present\n",
    "    if 'text' in df.columns:\n",
    "        df = df.drop(columns=['text'])\n",
    "    # Handle null STRUCTURE_IDs: set to -999 (best practice for consistency)\n",
    "    if 'STRUCTURE_ID' in df.columns:\n",
    "        df['STRUCTURE_ID'] = df['STRUCTURE_ID'].fillna(-999)\n",
    "    # Combine STRUCTURE_ID and COORDINATES into a unified identifier tuple\n",
    "    if 'STRUCTURE_ID' in df.columns and 'COORDINATES' in df.columns:\n",
    "        unified_id = list(zip(df['STRUCTURE_ID'], df['COORDINATES']))\n",
    "        df = df.drop(columns=['STRUCTURE_ID', 'COORDINATES'])\n",
    "        df.insert(0, 'unified_id', unified_id)\n",
    "    # Save the processed DataFrame\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "csv_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/merged_all.csv'\n",
    "output_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/final_processed.csv'\n",
    "final_df = finalize_merged_dataset(csv_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def split_final_dataset(schema_path, final_csv_path, output_folder):\n",
    "    \"\"\"\n",
    "    Using the master schema and the final processed CSV, create:\n",
    "    1) numerical.csv with unified_id + numerical/numerical_coded columns\n",
    "    2) categorical.csv with unified_id + nominal columns\n",
    "    3) text.csv with unified_id + tokens column\n",
    "    4) metadata.json with lists of numerical and categorical column names\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    df = pd.read_csv(final_csv_path)\n",
    "    if 'unified_id' not in df.columns:\n",
    "        raise ValueError(\"final_csv_path must contain 'unified_id' column\")\n",
    "    # Collect columns by type from schema\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "    for col, info in schema.items():\n",
    "        t = str(info.get('type', '')).lower()\n",
    "        if t in ['numerical', 'numerical_coded']:\n",
    "            numerical_cols.append(col)\n",
    "        elif t == 'nominal':\n",
    "            categorical_cols.append(col)\n",
    "    # Keep only columns that exist in the final CSV\n",
    "    numerical_cols = [c for c in numerical_cols if c in df.columns]\n",
    "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
    "    # 1) Numerical CSV\n",
    "    num_df = df[['unified_id'] + numerical_cols].copy()\n",
    "    num_df.to_csv(os.path.join(output_folder, 'numerical.csv'), index=False)\n",
    "    # 2) Categorical CSV\n",
    "    cat_df = df[['unified_id'] + categorical_cols].copy()\n",
    "    cat_df.to_csv(os.path.join(output_folder, 'categorical.csv'), index=False)\n",
    "    # 3) Text CSV\n",
    "    if 'tokens' not in df.columns:\n",
    "        raise ValueError(\"final_csv_path must contain 'tokens' column\")\n",
    "    text_df = df[['unified_id', 'tokens']].copy()\n",
    "    text_df.to_csv(os.path.join(output_folder, 'text.csv'), index=False)\n",
    "    # 4) Metadata JSON\n",
    "    metadata = {\n",
    "        'numerical_columns': numerical_cols,\n",
    "        'categorical_columns': categorical_cols\n",
    "    }\n",
    "    with open(os.path.join(output_folder, 'metadata.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    return num_df, cat_df, text_df, metadata\n",
    "\n",
    "# Example usage:\n",
    "schema_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/cleaned_master_schema.json'\n",
    "final_csv_path = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/final_processed.csv'\n",
    "output_folder = 'C:/Users/wongb/Bridge-ML/Bridge-ML-LLM-Embedding-Architecture/part1 clean/master data/segments/split'\n",
    "num_df, cat_df, text_df, metadata = split_final_dataset(schema_path, final_csv_path, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
