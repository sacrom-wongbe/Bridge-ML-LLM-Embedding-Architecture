{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiModalRowTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_feature_names,       # list[str]\n",
    "        cat_feature_names,       # list[str]\n",
    "        cat_vocab_sizes,         # list[int] matching order of cat_feature_names\n",
    "        text_vocab_size,         # size of tokenizer vocab (e.g., 30522 for BERT)\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        max_text_len: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Feature Bookkeeping\n",
    "        self.num_feature_names = num_feature_names\n",
    "        self.cat_feature_names = cat_feature_names\n",
    "        self.N_num = len(num_feature_names)\n",
    "        self.N_cat = len(cat_feature_names)\n",
    "        self.d_model = d_model\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "        # 2. THE TRINITY EMBEDDINGS\n",
    "        # Type Embeddings: 0=Numeric, 1=Categorical, 2=Text\n",
    "        self.type_emb = nn.Embedding(3, d_model)\n",
    "        \n",
    "        # Feature ID Embeddings: Unique ID for every column name\n",
    "        self.all_feat_names = num_feature_names + cat_feature_names\n",
    "        self.feature_to_id = {name: i for i, name in enumerate(self.all_feat_names)}\n",
    "        self.feature_emb = nn.Embedding(len(self.all_feat_names), d_model)\n",
    "\n",
    "        # 3. VALUE PROJECTIONS\n",
    "        # Numeric: Scalar -> Vector\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(self.N_num)])\n",
    "        \n",
    "        # Categorical: ID -> Vector\n",
    "        self.cat_val_emb = nn.ModuleList([\n",
    "            nn.Embedding(vsz, d_model) for vsz in cat_vocab_sizes\n",
    "        ])\n",
    "\n",
    "        # Text: Word ID -> Vector + Positional Embedding\n",
    "        self.text_tok_emb = nn.Embedding(text_vocab_size, d_model)\n",
    "        self.text_pos_emb = nn.Embedding(max_text_len, d_model)\n",
    "\n",
    "        # 4. THE GLOBAL AGGREGATOR (CLS Token)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        # 5. THE TRANSFORMER ENGINE\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, \n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 6. TASK HEADS\n",
    "        # Regression Head (For 10% labeled data)\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Reconstruction Heads (For Self-Supervised pre-training)\n",
    "        self.num_recon = nn.ModuleList([nn.Linear(d_model, 1) for _ in range(self.N_num)])\n",
    "        self.cat_recon = nn.ModuleList([nn.Linear(d_model, vsz) for vsz in cat_vocab_sizes])\n",
    "\n",
    "    def _build_sequence(self, num_x, cat_x, text_ids):\n",
    "        B = num_x.shape[0]\n",
    "        device = num_x.device\n",
    "        all_tokens = []\n",
    "\n",
    "        # Add CLS Token\n",
    "        all_tokens.append(self.cls_token.expand(B, -1, -1))\n",
    "\n",
    "        # Process Numeric\n",
    "        type_num = self.type_emb(torch.tensor([0], device=device))\n",
    "        for i, name in enumerate(self.num_feature_names):\n",
    "            val = self.num_proj[i](num_x[:, i].unsqueeze(-1))\n",
    "            feat = self.feature_emb(torch.tensor([self.feature_to_id[name]], device=device))\n",
    "            all_tokens.append((val + feat + type_num).unsqueeze(1))\n",
    "\n",
    "        # Process Categorical\n",
    "        type_cat = self.type_emb(torch.tensor([1], device=device))\n",
    "        for i, name in enumerate(self.cat_feature_names):\n",
    "            val = self.cat_val_emb[i](cat_x[:, i])\n",
    "            feat = self.feature_emb(torch.tensor([self.feature_to_id[name]], device=device))\n",
    "            all_tokens.append((val + feat + type_cat).unsqueeze(1))\n",
    "\n",
    "        # Process Text\n",
    "        if text_ids is not None:\n",
    "            type_txt = self.type_emb(torch.tensor([2], device=device))\n",
    "            txt_val = self.text_tok_emb(text_ids)\n",
    "            pos = self.text_pos_emb(torch.arange(text_ids.size(1), device=device))\n",
    "            all_tokens.append(txt_val + pos + type_txt)\n",
    "\n",
    "        return torch.cat(all_tokens, dim=1)\n",
    "\n",
    "    def forward(self, num_x, cat_x, text_ids=None, y=None, \n",
    "                num_mask=None, num_targets=None, cat_mask=None, cat_targets=None, \n",
    "                mode=\"regress\"):\n",
    "        \n",
    "        # 1. Tokenize and Run Transformer\n",
    "        X = self._build_sequence(num_x, cat_x, text_ids)\n",
    "        H = self.transformer(X) # (Batch, Seq_Len, d_model)\n",
    "\n",
    "        if mode == \"regress\":\n",
    "            # Pull the CLS token (index 0)\n",
    "            cls_out = H[:, 0, :]\n",
    "            y_hat = self.reg_head(cls_out).squeeze(-1)\n",
    "            loss = F.mse_loss(y_hat, y) if y is not None else None\n",
    "            return {\"y_hat\": y_hat, \"loss\": loss}\n",
    "\n",
    "        elif mode == \"pretrain\":\n",
    "            total_loss = 0\n",
    "            # Numeric Reconstruction Loss\n",
    "            H_num = H[:, 1 : 1+self.N_num, :]\n",
    "            for i in range(self.N_num):\n",
    "                mask = num_mask[:, i]\n",
    "                if mask.any():\n",
    "                    pred = self.num_recon[i](H_num[mask, i, :]).squeeze(-1)\n",
    "                    total_loss += F.mse_loss(pred, num_targets[mask, i])\n",
    "\n",
    "            # Categorical Reconstruction Loss\n",
    "            H_cat = H[:, 1+self.N_num : 1+self.N_num+self.N_cat, :]\n",
    "            for i in range(self.N_cat):\n",
    "                mask = cat_mask[:, i]\n",
    "                if mask.any():\n",
    "                    logits = self.cat_recon[i](H_cat[mask, i, :])\n",
    "                    total_loss += F.cross_entropy(logits, cat_targets[mask, i])\n",
    "            \n",
    "            return {\"loss\": total_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b769224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_masks(num_x, cat_x, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    num_x: (B, N_num) float tensor\n",
    "    cat_x: (B, N_cat) int64 tensor\n",
    "    \"\"\"\n",
    "    device = num_x.device\n",
    "    B, N_num = num_x.shape\n",
    "    _, N_cat = cat_x.shape\n",
    "\n",
    "    # 1. Create Numeric Masks\n",
    "    # Generate a random matrix, if value < prob, we mask it\n",
    "    num_mask = torch.rand(num_x.shape, device=device) < mask_prob\n",
    "    num_targets = num_x.clone()\n",
    "    \n",
    "    # We \"zero out\" the masked values in the input so the model can't see them\n",
    "    # Since num_x is normalized (mean 0), 0.0 is a neutral mask value\n",
    "    num_x_masked = num_x.clone()\n",
    "    num_x_masked[num_mask] = 0.0\n",
    "\n",
    "    # 2. Create Categorical Masks\n",
    "    cat_mask = torch.rand(cat_x.shape, device=device) < mask_prob\n",
    "    cat_targets = cat_x.clone()\n",
    "    \n",
    "    # We replace masked categories with ID 0 (the [MASK] token)\n",
    "    cat_x_masked = cat_x.clone()\n",
    "    cat_x_masked[cat_mask] = 0 \n",
    "\n",
    "    return num_x_masked, num_mask, num_targets, cat_x_masked, cat_mask, cat_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. SETUP DATASETS\n",
    "# Assume X_num, X_cat, X_text are tensors from your Preprocessing Notebook\n",
    "# total_dataset: all 4000 rows (for pre-training)\n",
    "# labeled_dataset: only the 400 rows + y_labels (for fine-tuning)\n",
    "train_loader_pre = DataLoader(total_dataset, batch_size=64, shuffle=True)\n",
    "train_loader_fine = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. INITIALIZE MODEL\n",
    "model = MultiModalRowTransformer(\n",
    "    num_feature_names=metadata['num_features'],\n",
    "    cat_feature_names=list(metadata['cat_features'].keys()),\n",
    "    cat_vocab_sizes=list(metadata['cat_features'].values()),\n",
    "    text_vocab_size=metadata['text_vocab_size'],\n",
    "    d_model=128\n",
    ").to('cuda')\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# --- PHASE 1: SELF-SUPERVISED PRE-TRAINING (The \"Masking\" Phase) ---\n",
    "print(\"Starting Pre-training...\")\n",
    "model.train()\n",
    "for epoch in range(50):  # Pre-train for more epochs\n",
    "    for batch in train_loader_pre:\n",
    "        num_x, cat_x, text_ids = [b.to('cuda') for b in batch]\n",
    "        \n",
    "        # Here you would generate masks (e.g., 15% random features)\n",
    "        # For simplicity, we assume you have a 'masking_function'\n",
    "        num_mask, num_targets, cat_mask, cat_targets = create_masks(num_x, cat_x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Run in PRETRAIN mode\n",
    "        out = model(\n",
    "            num_x, cat_x, text_ids, \n",
    "            num_mask=num_mask, num_targets=num_targets,\n",
    "            cat_mask=cat_mask, cat_targets=cat_targets,\n",
    "            mode=\"pretrain\"\n",
    "        )\n",
    "        \n",
    "        loss = out[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# --- PHASE 2: DOWNSTREAM FINE-TUNING (The \"Label\" Phase) ---\n",
    "print(\"Starting Fine-tuning...\")\n",
    "# Optional: model.encoder.requires_grad_(False) # Freeze encoder if 400 samples are too noisy\n",
    "for epoch in range(20):\n",
    "    for batch in train_loader_fine:\n",
    "        num_x, cat_x, text_ids, y = [b.to('cuda') for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Run in REGRESS mode\n",
    "        out = model(num_x, cat_x, text_ids, y=y, mode=\"regress\")\n",
    "        \n",
    "        loss = out[\"loss\"] # This is the MSE Loss for the 10% labels\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
