{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e3d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def append_coordinates(csv_path, schema_path, structure_coords_path, output_csv_path=None, output_schema_path=None):\n",
    "    \"\"\"\n",
    "    Ensure COORDINATES column is in the second position in both CSV and schema.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the target CSV file\n",
    "        schema_path: Path to the target JSON schema file\n",
    "        structure_coords_path: Path to structure_coordinates.csv\n",
    "        output_csv_path: Optional output path for CSV (defaults to input path)\n",
    "        output_schema_path: Optional output path for schema (defaults to input path)\n",
    "    \"\"\"\n",
    "    # Set default output paths\n",
    "    if output_csv_path is None:\n",
    "        output_csv_path = csv_path\n",
    "    if output_schema_path is None:\n",
    "        output_schema_path = schema_path\n",
    "    \n",
    "    # Load files\n",
    "    df = pd.read_csv(csv_path)\n",
    "    structure_coords = pd.read_csv(structure_coords_path)\n",
    "    \n",
    "    with open(schema_path, 'r') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    # Get the first column name (structure ID column)\n",
    "    id_column = df.columns[0]\n",
    "    \n",
    "    # Check if COORDINATES already exists\n",
    "    has_coordinates = 'COORDINATES' in df.columns\n",
    "    \n",
    "    if has_coordinates:\n",
    "        print(f\"COORDINATES column found in {csv_path}\")\n",
    "        current_position = list(df.columns).index('COORDINATES')\n",
    "        \n",
    "        if current_position != 1:\n",
    "            print(f\"Moving COORDINATES from position {current_position} to position 1 (second column)\")\n",
    "            # Reorder CSV columns\n",
    "            cols = list(df.columns)\n",
    "            cols.remove('COORDINATES')\n",
    "            cols.insert(1, 'COORDINATES')\n",
    "            df = df[cols]\n",
    "            \n",
    "            # Reorder schema\n",
    "            schema_items = list(schema.items())\n",
    "            coord_item = None\n",
    "            for i, (key, value) in enumerate(schema_items):\n",
    "                if key == 'COORDINATES':\n",
    "                    coord_item = schema_items.pop(i)\n",
    "                    break\n",
    "            if coord_item:\n",
    "                schema_items.insert(1, coord_item)\n",
    "                schema = dict(schema_items)\n",
    "        else:\n",
    "            print(\"COORDINATES is already in position 1 (second column)\")\n",
    "    else:\n",
    "        print(f\"COORDINATES column not found in {csv_path}. Adding from structure_coordinates.csv\")\n",
    "        \n",
    "        # Merge coordinates based on structure ID\n",
    "        df = pd.merge(df, structure_coords, left_on=id_column, right_on=structure_coords.columns[0], how='left')\n",
    "        \n",
    "        # If merge created a duplicate ID column, remove it\n",
    "        if len(df.columns) > len(df.columns.unique()):\n",
    "            # Find duplicate columns\n",
    "            for col in structure_coords.columns:\n",
    "                if col != 'COORDINATES' and col in df.columns:\n",
    "                    # Keep only the first occurrence\n",
    "                    df = df.loc[:, ~df.columns.duplicated()]\n",
    "        \n",
    "        # Reorder to put COORDINATES in second position\n",
    "        cols = list(df.columns)\n",
    "        cols.remove('COORDINATES')\n",
    "        cols.insert(1, 'COORDINATES')\n",
    "        df = df[cols]\n",
    "        \n",
    "        # Add COORDINATES to schema in second position\n",
    "        schema_items = list(schema.items())\n",
    "        coord_schema = (\"COORDINATES\", {\n",
    "            \"type\": \"reference\",\n",
    "            \"description\": None\n",
    "        })\n",
    "        schema_items.insert(1, coord_schema)\n",
    "        schema = dict(schema_items)\n",
    "    \n",
    "    # Save updated files\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Updated CSV saved to: {output_csv_path}\")\n",
    "    \n",
    "    with open(output_schema_path, 'w') as f:\n",
    "        json.dump(schema, f, indent=2)\n",
    "    print(f\"Updated schema saved to: {output_schema_path}\")\n",
    "    \n",
    "    return df, schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a90922",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Run the function below with your specific CSV and schema files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0fcd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COORDINATES column not found in C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\\nfhl_fema_flood.csv. Adding from structure_coordinates.csv\n",
      "Updated CSV saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\\nfhl_fema_flood.csv\n",
      "Updated schema saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\\nfhl_fema_schema_master.json\n",
      "\n",
      "First 5 rows of updated CSV:\n",
      "  STRUCTURE_ID                  COORDINATES NFHL_FLD_ZONE NFHL_SFHA  \\\n",
      "0               (47.98571667, -122.2271222)             X         F   \n",
      "1                    (47.697975, -122.6195)             X         F   \n",
      "2                  (48.21215, -121.9331306)             X         F   \n",
      "3               (47.56759167, -122.5517028)             X         F   \n",
      "4                  (47.769275, -122.707925)             X         F   \n",
      "\n",
      "   NFHL_STATIC_BFE NFHL_V_DATUM             NFHL_ZONE_SUBTYPE NFHL_SOURCE_CIT  \n",
      "0          -9999.0          NaN  AREA OF MINIMAL FLOOD HAZARD   53061C_STUDY1  \n",
      "1          -9999.0          NaN  AREA OF MINIMAL FLOOD HAZARD   53061C_STUDY1  \n",
      "2          -9999.0          NaN  AREA OF MINIMAL FLOOD HAZARD   53061C_STUDY1  \n",
      "3          -9999.0          NaN  AREA OF MINIMAL FLOOD HAZARD   53061C_STUDY1  \n",
      "4          -9999.0          NaN  AREA OF MINIMAL FLOOD HAZARD   53061C_STUDY1  \n",
      "\n",
      "Column order: ['STRUCTURE_ID', 'COORDINATES', 'NFHL_FLD_ZONE', 'NFHL_SFHA', 'NFHL_STATIC_BFE']...\n",
      "\n",
      "First 3 schema keys: ['STRUCTURE_ID', 'COORDINATES', 'NFHL_FLD_ZONE']\n"
     ]
    }
   ],
   "source": [
    "# Example: Process a single file\n",
    "csv_path = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\\nfhl_fema_flood.csv\"\n",
    "schema_path = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\\nfhl_fema_schema_master.json\"\n",
    "structure_coords_path = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\\structure_coordinates.csv\"\n",
    "\n",
    "# Run the function\n",
    "df, schema = append_coordinates(csv_path, schema_path, structure_coords_path)\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows of updated CSV:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumn order: {list(df.columns)[:5]}...\")\n",
    "print(f\"\\nFirst 3 schema keys: {list(schema.keys())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def get_base_name(filename):\n",
    "    \"\"\"\n",
    "    Remove the last suffix (after last underscore) from filename.\n",
    "    E.g., 'macrostrat_schema_bins.json' -> 'macrostrat_schema'\n",
    "         'nbi_nominal_schema_enhanced.json' -> 'nbi_nominal_schema'\n",
    "    \"\"\"\n",
    "    name_without_ext = filename.rsplit('.', 1)[0]  # Remove .json\n",
    "    parts = name_without_ext.rsplit('_', 1)  # Split on last underscore\n",
    "    return parts[0] if len(parts) > 1 else name_without_ext\n",
    "\n",
    "def find_matching_file(base_name, folder_path):\n",
    "    \"\"\"\n",
    "    Find a JSON file in folder_path that matches the base_name\n",
    "    (after removing its own last suffix).\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    if not folder.exists():\n",
    "        return None\n",
    "    \n",
    "    for file in folder.glob('*.json'):\n",
    "        if get_base_name(file.name) == base_name:\n",
    "            return file\n",
    "    return None\n",
    "\n",
    "def merge_schemas(schema1, schema2):\n",
    "    \"\"\"\n",
    "    Replace entries in schema1 with entries from schema2 that have \"type\": \"nominal\".\n",
    "    \n",
    "    Args:\n",
    "        schema1: Dict - base schema\n",
    "        schema2: Dict - schema with nominal entries to merge\n",
    "    \n",
    "    Returns:\n",
    "        Dict - merged schema\n",
    "    \"\"\"\n",
    "    merged = schema1.copy()\n",
    "    \n",
    "    for key, value in schema2.items():\n",
    "        if isinstance(value, dict) and value.get('type') == 'nominal':\n",
    "            merged[key] = value\n",
    "            print(f\"  - Replaced '{key}' with nominal type from second schema\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def synthesize_schemas(folder1_path, folder2_path, output_path):\n",
    "    \"\"\"\n",
    "    Synthesize schemas from two folders by matching base names and merging nominal types.\n",
    "    \n",
    "    Args:\n",
    "        folder1_path: Path to first folder with JSON schemas (base schemas)\n",
    "        folder2_path: Path to second folder with JSON schemas (schemas with nominal types)\n",
    "        output_path: Path to output folder for synthesized schemas\n",
    "    \"\"\"\n",
    "    folder1 = Path(folder1_path)\n",
    "    folder2 = Path(folder2_path)\n",
    "    output = Path(output_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all JSON files from folder1\n",
    "    json_files = list(folder1.glob('*.json'))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {folder1_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files in {folder1_path}\\n\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"Processing: {json_file.name}\")\n",
    "        base_name = get_base_name(json_file.name)\n",
    "        print(f\"  Base name: {base_name}\")\n",
    "        \n",
    "        # Load first schema\n",
    "        with open(json_file, 'r') as f:\n",
    "            schema1 = json.load(f)\n",
    "        \n",
    "        # Try to find matching file in folder2\n",
    "        matching_file = find_matching_file(base_name, folder2)\n",
    "        \n",
    "        if matching_file:\n",
    "            print(f\"  Match found: {matching_file.name}\")\n",
    "            \n",
    "            # Load second schema\n",
    "            with open(matching_file, 'r') as f:\n",
    "                schema2 = json.load(f)\n",
    "            \n",
    "            # Merge schemas\n",
    "            merged_schema = merge_schemas(schema1, schema2)\n",
    "            \n",
    "            # Save merged schema\n",
    "            output_file = output / json_file.name\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(merged_schema, f, indent=2)\n",
    "            print(f\"  Saved merged schema to: {output_file}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"  No match found in {folder2_path}\")\n",
    "            print(f\"  Copying original to output\")\n",
    "            \n",
    "            # Copy original file to output\n",
    "            output_file = output / json_file.name\n",
    "            shutil.copy(json_file, output_file)\n",
    "            print(f\"  Saved original schema to: {output_file}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Schema synthesis complete!\")\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1e62e",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Synthesize schemas from bin_schemas and codemap_schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18ae6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 JSON files in C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\bin_schemas\n",
      "\n",
      "============================================================\n",
      "Processing: macrostrat_schema_bins.json\n",
      "============================================================\n",
      "Base name: macrostrat_schema\n",
      "Match found: macrostrat_schema_enhanced.json\n",
      "  Total replacements: 0\n",
      "Saved synthesized schema to: macrostrat_schema_bins.json\n",
      "\n",
      "============================================================\n",
      "Processing: nbi_numerical_coded_schema_bins.json\n",
      "============================================================\n",
      "Base name: nbi_numerical_coded_schema\n",
      "No match found in folder2. Copying original to output.\n",
      "\n",
      "============================================================\n",
      "Processing: nbi_numerical_schema_bins.json\n",
      "============================================================\n",
      "Base name: nbi_numerical_schema\n",
      "No match found in folder2. Copying original to output.\n",
      "\n",
      "============================================================\n",
      "Processing: nfhl_fema_schema_bins.json\n",
      "============================================================\n",
      "Base name: nfhl_fema_schema\n",
      "Match found: nfhl_fema_schema_enhanced.json\n",
      "  - Replaced 'NFHL_FLD_ZONE' with nominal type from second folder\n",
      "  - Replaced 'NFHL_SFHA' with nominal type from second folder\n",
      "  Total replacements: 2\n",
      "Saved synthesized schema to: nfhl_fema_schema_bins.json\n",
      "\n",
      "============================================================\n",
      "Processing: nhsm_hazard_schema_bins.json\n",
      "============================================================\n",
      "Base name: nhsm_hazard_schema\n",
      "Match found: nhsm_hazard_schema_enhanced.json\n",
      "  Total replacements: 0\n",
      "Saved synthesized schema to: nhsm_hazard_schema_bins.json\n",
      "\n",
      "============================================================\n",
      "Processing: usgs_design_maps_schema_bins.json\n",
      "============================================================\n",
      "Base name: usgs_design_maps_schema\n",
      "Match found: usgs_design_maps_schema_enhanced.json\n",
      "  - Replaced 'SDCS' with nominal type from second folder\n",
      "  Total replacements: 1\n",
      "Saved synthesized schema to: usgs_design_maps_schema_bins.json\n",
      "\n",
      "============================================================\n",
      "Schema synthesis complete! Output saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: Merge bin_schemas with codemap_schemas\n",
    "folder1 = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\bin_schemas\"\n",
    "folder2 = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\codemap_schemas\"\n",
    "output = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\"\n",
    "\n",
    "synthesize_schemas(folder1, folder2, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d564d",
   "metadata": {},
   "source": [
    "## Test the matching logic\n",
    "\n",
    "See how files will be matched before running the full synthesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd55b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test matching logic\n",
    "folder1 = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\bin_schemas\"\n",
    "folder2 = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\codemap_schemas\"\n",
    "\n",
    "folder1_path = Path(folder1)\n",
    "folder2_path = Path(folder2)\n",
    "\n",
    "print(\"File Matching Preview:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for json_file in folder1_path.glob('*.json'):\n",
    "    base_name = get_base_name(json_file.name)\n",
    "    matching_file = find_matching_file(base_name, folder2)\n",
    "    \n",
    "    print(f\"\\nFile 1: {json_file.name}\")\n",
    "    print(f\"  Base name: {base_name}\")\n",
    "    if matching_file:\n",
    "        print(f\"  ✓ Match: {matching_file.name}\")\n",
    "    else:\n",
    "        print(f\"  ✗ No match found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
