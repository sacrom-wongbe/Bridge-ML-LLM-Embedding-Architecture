{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5746e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import random\n",
    "\n",
    "def print_section(title, char=\"=\", width=80):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(title.center(width))\n",
    "    print(f\"{char * width}\\n\")\n",
    "\n",
    "def find_column_index(df: pd.DataFrame, column_name: str) -> int:\n",
    "    \"\"\"Find the index of a column by name.\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df.columns.get_loc(column_name)\n",
    "    raise ValueError(f\"Column '{column_name}' not found in dataframe\")\n",
    "\n",
    "def get_numerical_bin_sentence(value: float, edges: List, sentences: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find which bin the value falls into and return corresponding sentence.\n",
    "    edges = [0, 20, 50, 80, \"inf\"] creates 4 bins:\n",
    "      - bin 0: [0, 20)\n",
    "      - bin 1: [20, 50)\n",
    "      - bin 2: [50, 80)\n",
    "      - bin 3: [80, inf)\n",
    "    \"\"\"\n",
    "    # Convert \"inf\" strings to float('inf')\n",
    "    edges_numeric = []\n",
    "    for e in edges:\n",
    "        if isinstance(e, str) and e.lower() in ['inf', '-inf']:\n",
    "            edges_numeric.append(float(e))\n",
    "        else:\n",
    "            edges_numeric.append(float(e))\n",
    "    \n",
    "    # Find bin index\n",
    "    bin_index = 0\n",
    "    for i in range(len(edges_numeric) - 1):\n",
    "        if value >= edges_numeric[i] and value < edges_numeric[i + 1]:\n",
    "            bin_index = i\n",
    "            break\n",
    "        elif i == len(edges_numeric) - 2:  # Last bin (to infinity)\n",
    "            bin_index = i\n",
    "    \n",
    "    return sentences[bin_index]\n",
    "\n",
    "def ensure_period(text: str) -> str:\n",
    "    \"\"\"Ensure text ends with a period.\"\"\"\n",
    "    text = text.strip()\n",
    "    if text and not text.endswith('.'):\n",
    "        text += '.'\n",
    "    return text\n",
    "\n",
    "def generate_sentence(column_name: str, value: Any, schema_field: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full sentence for a column value based on schema rules.\n",
    "    Returns: \"{title}: {sentence}\"\n",
    "    \"\"\"\n",
    "    # Skip null/missing values\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    title = schema_field.get('title', column_name)\n",
    "    field_type = schema_field.get('type')\n",
    "    \n",
    "    if field_type == 'nominal':\n",
    "        # Match value to code_map\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        if sentence:\n",
    "            return f\"{title}: {ensure_period(sentence)}\"\n",
    "    \n",
    "    elif field_type == 'nl':\n",
    "        # Use raw value as sentence\n",
    "        return f\"{title}: {ensure_period(str(value))}\"\n",
    "    \n",
    "    elif field_type == 'numerical':\n",
    "        # Use semantic bins\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        sentences = semantic_bins.get('sentences', [])\n",
    "        \n",
    "        if edges and sentences:\n",
    "            try:\n",
    "                sentence = get_numerical_bin_sentence(float(value), edges, sentences)\n",
    "                return f\"{title}: {ensure_period(sentence)}\"\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    elif field_type == 'numerical_coded':\n",
    "        # Try code_map first, fallback to semantic bins\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        \n",
    "        if sentence:\n",
    "            return f\"{title}: {ensure_period(sentence)}\"\n",
    "        \n",
    "        # Fallback to semantic bins\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        sentences = semantic_bins.get('sentences', [])\n",
    "        \n",
    "        if edges and sentences:\n",
    "            try:\n",
    "                sentence = get_numerical_bin_sentence(float(value), edges, sentences)\n",
    "                return f\"{title}: {ensure_period(sentence)}\"\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical_coded value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_schema_csv_pairs(schema_folder: Path, data_folder: Path) -> List[Tuple[Dict, pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Load matching schema/CSV pairs.\n",
    "    Returns: List of (schema_dict, dataframe, source_name) tuples\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Get all schema files\n",
    "    schema_files = sorted(schema_folder.glob(\"*_schema_master.json\"))\n",
    "    \n",
    "    for schema_file in schema_files:\n",
    "        # Derive CSV filename\n",
    "        csv_name = schema_file.stem.replace(\"_schema_master\", \"\") + \".csv\"\n",
    "        csv_path = data_folder / csv_name\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"âš ï¸  Warning: No matching CSV for {schema_file.name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load schema\n",
    "        with open(schema_file, 'r', encoding='utf-8') as f:\n",
    "            schema = json.load(f)\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        source_name = csv_name.replace(\".csv\", \"\")\n",
    "        pairs.append((schema, df, source_name))\n",
    "        print(f\"âœ… Loaded: {schema_file.name} + {csv_name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_paragraph_for_row(row_index: int, schema_csv_pairs: List[Tuple[Dict, pd.DataFrame, str]]) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generate a paragraph for a single row across all CSVs.\n",
    "    Returns: (structure_id, coordinates, paragraph)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    structure_id = None\n",
    "    coordinates = None\n",
    "    \n",
    "    for schema, df, source_name in schema_csv_pairs:\n",
    "        row = df.iloc[row_index]\n",
    "        \n",
    "        # Extract structure_id and coordinates from first pair\n",
    "        if structure_id is None:\n",
    "            structure_id = row.iloc[0] if 'STRUCTURE_ID' not in df.columns else row['STRUCTURE_ID']\n",
    "        if coordinates is None:\n",
    "            coordinates = row.iloc[1] if 'COORDINATES' not in df.columns else row['COORDINATES']\n",
    "        \n",
    "        # Process each column (skip STRUCTURE_ID and COORDINATES)\n",
    "        for column_name, schema_field in schema.items():\n",
    "            if column_name in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "                continue\n",
    "            \n",
    "            # Skip reference and redundant types\n",
    "            field_type = schema_field.get('type')\n",
    "            if field_type in ['reference', 'numerical_redundant', 'nominal_redundant', 'numerical_coded_redundant']:\n",
    "                continue\n",
    "            \n",
    "            if column_name not in df.columns:\n",
    "                print(f\"    âš ï¸  Column mismatch: '{column_name}' in schema but not in {source_name}.csv\")\n",
    "                raise ValueError(f\"Column mismatch: {column_name}\")\n",
    "            \n",
    "            value = row[column_name]\n",
    "            sentence = generate_sentence(column_name, value, schema_field)\n",
    "            \n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    # Combine all sentences into one paragraph\n",
    "    paragraph = \" \".join(sentences)\n",
    "    \n",
    "    return str(structure_id), str(coordinates), paragraph\n",
    "\n",
    "def test_random_paragraph(schema_folder: str, data_folder: str, output_folder: str = \"logs\"):\n",
    "    \"\"\"\n",
    "    Generate a paragraph for a random bridge and save to log file.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    print_section(\"TEST MODE - Random Paragraph Generation\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Get row count from first dataframe\n",
    "    num_rows = len(pairs[0][1])\n",
    "    print(f\"\\nğŸ“Š Total rows available: {num_rows:,}\")\n",
    "    \n",
    "    # Pick random row\n",
    "    random_row = random.randint(0, num_rows - 1)\n",
    "    print(f\"ğŸ² Selected random row index: {random_row}\")\n",
    "    \n",
    "    # Generate paragraph\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Generating paragraph...\")\n",
    "    print(f\"{'â”€' * 80}\\n\")\n",
    "    \n",
    "    structure_id, coordinates, paragraph = generate_paragraph_for_row(random_row, pairs)\n",
    "    \n",
    "    # Save to log\n",
    "    log_file = output_folder / f\"test_paragraph_{structure_id}.txt\"\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STRUCTURE_ID: {structure_id}\\n\")\n",
    "        f.write(f\"COORDINATES: {coordinates}\\n\")\n",
    "        f.write(f\"ROW_INDEX: {random_row}\\n\")\n",
    "        f.write(f\"\\n{'=' * 80}\\n\")\n",
    "        f.write(f\"PARAGRAPH:\\n\")\n",
    "        f.write(f\"{'=' * 80}\\n\\n\")\n",
    "        f.write(paragraph)\n",
    "    \n",
    "    print(f\"âœ… Saved to: {log_file}\")\n",
    "    print(f\"\\nğŸ“ Paragraph length: {len(paragraph):,} characters\")\n",
    "    print(f\"ğŸ“ Word count: {len(paragraph.split()):,} words\")\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Preview (first 500 characters):\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(paragraph[:500] + \"...\\n\")\n",
    "\n",
    "def generate_all_paragraphs(schema_folder: str, data_folder: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Generate paragraphs for all bridges and save to CSV.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    print_section(\"GENERATING PARAGRAPHS FOR ALL BRIDGES\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Validate row counts match\n",
    "    print(\"\\nğŸ“Š Validating row counts...\")\n",
    "    row_counts = [len(df) for _, df, _ in pairs]\n",
    "    if len(set(row_counts)) > 1:\n",
    "        print(\"âŒ ERROR: Row count mismatch across CSVs!\")\n",
    "        for schema, df, name in pairs:\n",
    "            print(f\"   {name}: {len(df):,} rows\")\n",
    "        raise ValueError(\"Row count mismatch\")\n",
    "    \n",
    "    num_rows = row_counts[0]\n",
    "    print(f\"âœ… All CSVs have {num_rows:,} rows\")\n",
    "    \n",
    "    # Generate paragraphs\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Generating paragraphs...\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for row_idx in range(num_rows):\n",
    "        if row_idx % 100 == 0:\n",
    "            print(f\"Progress: {row_idx:,} / {num_rows:,} ({100 * row_idx / num_rows:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            structure_id, coordinates, paragraph = generate_paragraph_for_row(row_idx, pairs)\n",
    "            results.append({\n",
    "                'STRUCTURE_ID': structure_id,\n",
    "                'COORDINATES': coordinates,\n",
    "                'PARAGRAPH': paragraph\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error at row {row_idx}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"\\nğŸ’¾ Saving to {output_path}...\")\n",
    "    df_output = pd.DataFrame(results)\n",
    "    df_output.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print_section(\"COMPLETE\")\n",
    "    print(f\"âœ… Generated {len(results):,} paragraphs\")\n",
    "\n",
    "    print(f\"ğŸ’¾ Saved to: {output_path}\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")\n",
    "\n",
    "    print(f\"\\nğŸ“ Statistics:\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")\n",
    "\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Max paragraph length: {df_output['PARAGRAPH'].str.len().max():,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35778998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    TEST MODE - Random Paragraph Generation                     \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Total rows available: 4,914\n",
      "ğŸ² Selected random row index: 456\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Generating paragraph...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… Saved to: logs\\test_paragraph_0005142A.txt\n",
      "\n",
      "ğŸ“ Paragraph length: 21,808 characters\n",
      "ğŸ“ Word count: 2,923 words\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Preview (first 500 characters):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Design Peak Ground Acceleration: High seismic demand bridges with PGA values between 0.4 and 0.6 must incorporate advanced engineering techniques to withstand significant seismic forces and prevent structural damage. Short-Period Spectral Acceleration: High seismic demand on bridges, indicated by SS values between 1.0 and 1.5, necessitates advanced seismic design strategies, including the use of ductile materials and energy dissipation systems to maintain structural integrity. Spectral Accelerat...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test mode - generate one random paragraph\n",
    "test_random_paragraph(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad6be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                     GENERATING PARAGRAPHS FOR ALL BRIDGES                      \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Validating row counts...\n",
      "âœ… All CSVs have 4,914 rows\n",
      "\n",
      "================================================================================\n",
      "Generating paragraphs...\n",
      "================================================================================\n",
      "\n",
      "Progress: 0 / 4,914 (0.0%)\n",
      "Progress: 100 / 4,914 (2.0%)\n",
      "Progress: 200 / 4,914 (4.1%)\n",
      "Progress: 300 / 4,914 (6.1%)\n",
      "Progress: 400 / 4,914 (8.1%)\n",
      "Progress: 500 / 4,914 (10.2%)\n",
      "Progress: 600 / 4,914 (12.2%)\n",
      "Progress: 700 / 4,914 (14.2%)\n",
      "Progress: 800 / 4,914 (16.3%)\n",
      "Progress: 900 / 4,914 (18.3%)\n",
      "Progress: 1,000 / 4,914 (20.4%)\n",
      "Progress: 1,100 / 4,914 (22.4%)\n",
      "Progress: 1,200 / 4,914 (24.4%)\n",
      "Progress: 1,300 / 4,914 (26.5%)\n",
      "Progress: 1,400 / 4,914 (28.5%)\n",
      "Progress: 1,500 / 4,914 (30.5%)\n",
      "Progress: 1,600 / 4,914 (32.6%)\n",
      "Progress: 1,700 / 4,914 (34.6%)\n",
      "Progress: 1,800 / 4,914 (36.6%)\n",
      "Progress: 1,900 / 4,914 (38.7%)\n",
      "Progress: 2,000 / 4,914 (40.7%)\n",
      "Progress: 2,100 / 4,914 (42.7%)\n",
      "Progress: 2,200 / 4,914 (44.8%)\n",
      "Progress: 2,300 / 4,914 (46.8%)\n",
      "Progress: 2,400 / 4,914 (48.8%)\n",
      "Progress: 2,500 / 4,914 (50.9%)\n",
      "Progress: 2,600 / 4,914 (52.9%)\n",
      "Progress: 2,700 / 4,914 (54.9%)\n",
      "Progress: 2,800 / 4,914 (57.0%)\n",
      "Progress: 2,900 / 4,914 (59.0%)\n",
      "Progress: 3,000 / 4,914 (61.1%)\n",
      "Progress: 3,100 / 4,914 (63.1%)\n",
      "Progress: 3,200 / 4,914 (65.1%)\n",
      "Progress: 3,300 / 4,914 (67.2%)\n",
      "Progress: 3,400 / 4,914 (69.2%)\n",
      "Progress: 3,500 / 4,914 (71.2%)\n",
      "Progress: 3,600 / 4,914 (73.3%)\n",
      "Progress: 3,700 / 4,914 (75.3%)\n",
      "Progress: 3,800 / 4,914 (77.3%)\n",
      "Progress: 3,900 / 4,914 (79.4%)\n",
      "Progress: 4,000 / 4,914 (81.4%)\n",
      "Progress: 4,100 / 4,914 (83.4%)\n",
      "Progress: 4,200 / 4,914 (85.5%)\n",
      "Progress: 4,300 / 4,914 (87.5%)\n",
      "Progress: 4,400 / 4,914 (89.5%)\n",
      "Progress: 4,500 / 4,914 (91.6%)\n",
      "Progress: 4,600 / 4,914 (93.6%)\n",
      "Progress: 4,700 / 4,914 (95.6%)\n",
      "Progress: 4,800 / 4,914 (97.7%)\n",
      "Progress: 4,900 / 4,914 (99.7%)\n",
      "\n",
      "ğŸ’¾ Saving to C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\nlp_data\\bridge_paragraphs.csv...\n",
      "\n",
      "================================================================================\n",
      "                                    COMPLETE                                    \n",
      "================================================================================\n",
      "\n",
      "âœ… Generated 4,914 paragraphs\n",
      "ğŸ’¾ Saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\nlp_data\\bridge_paragraphs.csv\n",
      "\n",
      "ğŸ“ Statistics:\n",
      "   Average paragraph length: 21794 characters\n",
      "   Max paragraph length: 23,344 characters\n",
      "   Min paragraph length: 19,167 characters\n"
     ]
    }
   ],
   "source": [
    "# Full generation\n",
    "generate_all_paragraphs(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\",\n",
    "    output_file=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\nlp_data\\bridge_paragraphs.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464a1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_label(value: float, edges: List, labels: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find which bin the value falls into and return corresponding label.\n",
    "    Uses labels instead of sentences for snappier output.\n",
    "    \"\"\"\n",
    "    # Convert \"inf\" strings to float('inf')\n",
    "    edges_numeric = []\n",
    "    for e in edges:\n",
    "        if isinstance(e, str) and e.lower() in ['inf', '-inf']:\n",
    "            edges_numeric.append(float(e))\n",
    "        else:\n",
    "            edges_numeric.append(float(e))\n",
    "    \n",
    "    # Find bin index\n",
    "    bin_index = 0\n",
    "    for i in range(len(edges_numeric) - 1):\n",
    "        if value >= edges_numeric[i] and value < edges_numeric[i + 1]:\n",
    "            bin_index = i\n",
    "            break\n",
    "        elif i == len(edges_numeric) - 2:  # Last bin (to infinity)\n",
    "            bin_index = i\n",
    "    \n",
    "    return labels[bin_index]\n",
    "\n",
    "def generate_sentence_snappy(column_name: str, value: Any, schema_field: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a snappier sentence using labels instead of full sentences.\n",
    "    Returns: \"{title_snake_case}: {label}\" in lowercase\n",
    "    \"\"\"\n",
    "    # Skip null/missing values\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    title = schema_field.get('title', column_name)\n",
    "    # Convert title to snake_case\n",
    "    title_snake = title.replace(' ', '_')\n",
    "    \n",
    "    field_type = schema_field.get('type')\n",
    "    \n",
    "    if field_type == 'nominal':\n",
    "        # Match value to code_map\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        if sentence:\n",
    "            return f\"{title_snake}: {ensure_period(sentence)}\".lower()\n",
    "    \n",
    "    elif field_type == 'nl':\n",
    "        # Use raw value as sentence (unchanged)\n",
    "        return f\"{title_snake}: {ensure_period(str(value))}\".lower()\n",
    "    \n",
    "    elif field_type == 'numerical':\n",
    "        # Use labels instead of sentences\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        labels = semantic_bins.get('labels', [])\n",
    "        \n",
    "        if edges and labels:\n",
    "            try:\n",
    "                label = get_numerical_label(float(value), edges, labels)\n",
    "                return f\"{title_snake}: {ensure_period(label)}\".lower()\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    elif field_type == 'numerical_coded':\n",
    "        # Check code_map first - if value is in code_map, skip it\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        if str(value) in code_map or (isinstance(value, (int, float)) and str(int(value)) in code_map):\n",
    "            return None  # Skip coded values\n",
    "        \n",
    "        # Otherwise use labels like numerical\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        labels = semantic_bins.get('labels', [])\n",
    "        \n",
    "        if edges and labels:\n",
    "            try:\n",
    "                label = get_numerical_label(float(value), edges, labels)\n",
    "                return f\"{title_snake}: {ensure_period(label)}\".lower()\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical_coded value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def generate_paragraph_snappy(row_index: int, schema_csv_pairs: List[Tuple[Dict, pd.DataFrame, str]]) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generate a snappier paragraph for a single row across all CSVs.\n",
    "    Returns: (structure_id, coordinates, paragraph)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    structure_id = None\n",
    "    coordinates = None\n",
    "    \n",
    "    for schema, df, source_name in schema_csv_pairs:\n",
    "        row = df.iloc[row_index]\n",
    "        \n",
    "        # Extract structure_id and coordinates from first pair\n",
    "        if structure_id is None:\n",
    "            structure_id = row.iloc[0] if 'STRUCTURE_ID' not in df.columns else row['STRUCTURE_ID']\n",
    "        if coordinates is None:\n",
    "            coordinates = row.iloc[1] if 'COORDINATES' not in df.columns else row['COORDINATES']\n",
    "        \n",
    "        # Process each column (skip STRUCTURE_ID and COORDINATES)\n",
    "        for column_name, schema_field in schema.items():\n",
    "            if column_name in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "                continue\n",
    "            \n",
    "            # Skip reference and redundant types\n",
    "            field_type = schema_field.get('type')\n",
    "            if field_type in ['reference', 'numerical_redundant', 'nominal_redundant', 'numerical_coded_redundant']:\n",
    "                continue\n",
    "            \n",
    "            if column_name not in df.columns:\n",
    "                print(f\"    âš ï¸  Column mismatch: '{column_name}' in schema but not in {source_name}.csv\")\n",
    "                raise ValueError(f\"Column mismatch: {column_name}\")\n",
    "            \n",
    "            value = row[column_name]\n",
    "            sentence = generate_sentence_snappy(column_name, value, schema_field)\n",
    "            \n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    # Combine all sentences into one paragraph\n",
    "    paragraph = \" \".join(sentences)\n",
    "    \n",
    "    return str(structure_id), str(coordinates), paragraph\n",
    "\n",
    "def generate_all_paragraphs_snappy(schema_folder: str, data_folder: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Generate snappier paragraphs for all bridges and save to CSV.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    print_section(\"GENERATING SNAPPY PARAGRAPHS FOR ALL BRIDGES\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Validate row counts match\n",
    "    print(\"\\nğŸ“Š Validating row counts...\")\n",
    "    row_counts = [len(df) for _, df, _ in pairs]\n",
    "    if len(set(row_counts)) > 1:\n",
    "        print(\"âŒ ERROR: Row count mismatch across CSVs!\")\n",
    "        for schema, df, name in pairs:\n",
    "            print(f\"   {name}: {len(df):,} rows\")\n",
    "        raise ValueError(\"Row count mismatch\")\n",
    "    \n",
    "    num_rows = row_counts[0]\n",
    "    print(f\"âœ… All CSVs have {num_rows:,} rows\")\n",
    "    \n",
    "    # Generate paragraphs\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Generating snappy paragraphs...\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for row_idx in range(num_rows):\n",
    "        if row_idx % 100 == 0:\n",
    "            print(f\"Progress: {row_idx:,} / {num_rows:,} ({100 * row_idx / num_rows:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            structure_id, coordinates, paragraph = generate_paragraph_snappy(row_idx, pairs)\n",
    "            results.append({\n",
    "                'STRUCTURE_ID': structure_id,\n",
    "                'COORDINATES': coordinates,\n",
    "                'PARAGRAPH': paragraph\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error at row {row_idx}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"\\nğŸ’¾ Saving to {output_path}...\")\n",
    "    df_output = pd.DataFrame(results)\n",
    "    df_output.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print_section(\"COMPLETE\")\n",
    "    print(f\"âœ… Generated {len(results):,} paragraphs\")\n",
    "    print(f\"ğŸ’¾ Saved to: {output_path}\")\n",
    "    print(f\"\\nğŸ“ Statistics:\")\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Max paragraph length: {df_output['PARAGRAPH'].str.len().max():,} characters\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b93cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                  GENERATING SNAPPY PARAGRAPHS FOR ALL BRIDGES                  \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Validating row counts...\n",
      "âœ… All CSVs have 4,914 rows\n",
      "\n",
      "================================================================================\n",
      "Generating snappy paragraphs...\n",
      "================================================================================\n",
      "\n",
      "Progress: 0 / 4,914 (0.0%)\n",
      "Progress: 100 / 4,914 (2.0%)\n",
      "Progress: 200 / 4,914 (4.1%)\n",
      "Progress: 300 / 4,914 (6.1%)\n",
      "Progress: 400 / 4,914 (8.1%)\n",
      "Progress: 500 / 4,914 (10.2%)\n",
      "Progress: 600 / 4,914 (12.2%)\n",
      "Progress: 700 / 4,914 (14.2%)\n",
      "Progress: 800 / 4,914 (16.3%)\n",
      "Progress: 900 / 4,914 (18.3%)\n",
      "Progress: 1,000 / 4,914 (20.4%)\n",
      "Progress: 1,100 / 4,914 (22.4%)\n",
      "Progress: 1,200 / 4,914 (24.4%)\n",
      "Progress: 1,300 / 4,914 (26.5%)\n",
      "Progress: 1,400 / 4,914 (28.5%)\n",
      "Progress: 1,500 / 4,914 (30.5%)\n",
      "Progress: 1,600 / 4,914 (32.6%)\n",
      "Progress: 1,700 / 4,914 (34.6%)\n",
      "Progress: 1,800 / 4,914 (36.6%)\n",
      "Progress: 1,900 / 4,914 (38.7%)\n",
      "Progress: 2,000 / 4,914 (40.7%)\n",
      "Progress: 2,100 / 4,914 (42.7%)\n",
      "Progress: 2,200 / 4,914 (44.8%)\n",
      "Progress: 2,300 / 4,914 (46.8%)\n",
      "Progress: 2,400 / 4,914 (48.8%)\n",
      "Progress: 2,500 / 4,914 (50.9%)\n",
      "Progress: 2,600 / 4,914 (52.9%)\n",
      "Progress: 2,700 / 4,914 (54.9%)\n",
      "Progress: 2,800 / 4,914 (57.0%)\n",
      "Progress: 2,900 / 4,914 (59.0%)\n",
      "Progress: 3,000 / 4,914 (61.1%)\n",
      "Progress: 3,100 / 4,914 (63.1%)\n",
      "Progress: 3,200 / 4,914 (65.1%)\n",
      "Progress: 3,300 / 4,914 (67.2%)\n",
      "Progress: 3,400 / 4,914 (69.2%)\n",
      "Progress: 3,500 / 4,914 (71.2%)\n",
      "Progress: 3,600 / 4,914 (73.3%)\n",
      "Progress: 3,700 / 4,914 (75.3%)\n",
      "Progress: 3,800 / 4,914 (77.3%)\n",
      "Progress: 3,900 / 4,914 (79.4%)\n",
      "Progress: 4,000 / 4,914 (81.4%)\n",
      "Progress: 4,100 / 4,914 (83.4%)\n",
      "Progress: 4,200 / 4,914 (85.5%)\n",
      "Progress: 4,300 / 4,914 (87.5%)\n",
      "Progress: 4,400 / 4,914 (89.5%)\n",
      "Progress: 4,500 / 4,914 (91.6%)\n",
      "Progress: 4,600 / 4,914 (93.6%)\n",
      "Progress: 4,700 / 4,914 (95.6%)\n",
      "Progress: 4,800 / 4,914 (97.7%)\n",
      "Progress: 4,900 / 4,914 (99.7%)\n",
      "\n",
      "ğŸ’¾ Saving to C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_short.csv...\n",
      "\n",
      "================================================================================\n",
      "                                    COMPLETE                                    \n",
      "================================================================================\n",
      "\n",
      "âœ… Generated 4,914 paragraphs\n",
      "ğŸ’¾ Saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_short.csv\n",
      "\n",
      "ğŸ“ Statistics:\n",
      "   Average paragraph length: 4181 characters\n",
      "   Max paragraph length: 5,162 characters\n",
      "   Min paragraph length: 3,336 characters\n"
     ]
    }
   ],
   "source": [
    "generate_all_paragraphs_snappy(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\",\n",
    "    output_file=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\sentence_data\\sentences_short.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e6cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nominal_codemaps(current_schema_path: str, old_schema_path: str, output_schema_path: str):\n",
    "    \"\"\"\n",
    "    Update nominal code_maps in current schema with code_maps from old schema.\n",
    "    Finds corresponding columns and replaces code_map for nominal type fields.\n",
    "    \"\"\"\n",
    "    # Load schemas\n",
    "    with open(current_schema_path, 'r', encoding='utf-8') as f:\n",
    "        current_schema = json.load(f)\n",
    "    \n",
    "    with open(old_schema_path, 'r', encoding='utf-8') as f:\n",
    "        old_schema = json.load(f)\n",
    "    \n",
    "    print(f\"ğŸ“– Loaded current schema: {Path(current_schema_path).name}\")\n",
    "    print(f\"ğŸ“– Loaded old schema: {Path(old_schema_path).name}\")\n",
    "    print()\n",
    "    \n",
    "    updated_count = 0\n",
    "    \n",
    "    # Iterate through current schema\n",
    "    for column_name, field_info in current_schema.items():\n",
    "        # Check if field is nominal type\n",
    "        if field_info.get('type') == 'nominal':\n",
    "            # Check if column exists in old schema\n",
    "            if column_name in old_schema:\n",
    "                old_field = old_schema[column_name]\n",
    "                \n",
    "                # Check if old schema has code_map\n",
    "                if 'code_map' in old_field:\n",
    "                    # Replace code_map\n",
    "                    current_schema[column_name]['code_map'] = old_field['code_map']\n",
    "                    updated_count += 1\n",
    "                    print(f\"âœ… Updated '{column_name}' code_map\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸  No code_map in old schema for '{column_name}'\")\n",
    "            else:\n",
    "                print(f\"âš ï¸  Column '{column_name}' not found in old schema\")\n",
    "    \n",
    "    # Save updated schema\n",
    "    with open(output_schema_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(current_schema, f, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"âœ… Updated {updated_count} nominal code_maps\")\n",
    "    print(f\"ğŸ’¾ Saved to: {output_schema_path}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a3ac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Loaded current schema: nbi_nominal_schema_master.json\n",
      "ğŸ“– Loaded old schema: nbi_nominal_schema.json\n",
      "\n",
      "âœ… Updated 'STATE_CODE_001' code_map\n",
      "âœ… Updated 'COUNTY_CODE_003' code_map\n",
      "âœ… Updated 'HIGHWAY_DISTRICT_002' code_map\n",
      "âœ… Updated 'ROUTE_PREFIX_005B' code_map\n",
      "âœ… Updated 'SERVICE_LEVEL_005C' code_map\n",
      "âœ… Updated 'BASE_HWY_NETWORK_012' code_map\n",
      "âœ… Updated 'TOLL_020' code_map\n",
      "âœ… Updated 'MAINTENANCE_021' code_map\n",
      "âœ… Updated 'FUNCTIONAL_CLASS_026' code_map\n",
      "âœ… Updated 'DESIGN_LOAD_031' code_map\n",
      "âœ… Updated 'MEDIAN_CODE_033' code_map\n",
      "âœ… Updated 'STRUCTURE_FLARED_035' code_map\n",
      "âœ… Updated 'RAILINGS_036A' code_map\n",
      "âœ… Updated 'TRANSITIONS_036B' code_map\n",
      "âœ… Updated 'APPR_RAIL_036C' code_map\n",
      "âœ… Updated 'APPR_RAIL_END_036D' code_map\n",
      "âœ… Updated 'HISTORY_037' code_map\n",
      "âœ… Updated 'NAVIGATION_038' code_map\n",
      "âœ… Updated 'OPEN_CLOSED_POSTED_041' code_map\n",
      "âœ… Updated 'SERVICE_ON_042A' code_map\n",
      "âœ… Updated 'SERVICE_UND_042B' code_map\n",
      "âœ… Updated 'STRUCTURE_KIND_043A' code_map\n",
      "âœ… Updated 'STRUCTURE_TYPE_043B' code_map\n",
      "âœ… Updated 'APPR_KIND_044A' code_map\n",
      "âœ… Updated 'APPR_TYPE_044B' code_map\n",
      "âœ… Updated 'VERT_CLR_UND_REF_054A' code_map\n",
      "âœ… Updated 'LAT_UND_REF_055A' code_map\n",
      "âœ… Updated 'DECK_COND_058' code_map\n",
      "âœ… Updated 'SUPERSTRUCTURE_COND_059' code_map\n",
      "âœ… Updated 'SUBSTRUCTURE_COND_060' code_map\n",
      "âœ… Updated 'CHANNEL_COND_061' code_map\n",
      "âœ… Updated 'OPR_RATING_METH_063' code_map\n",
      "âœ… Updated 'INV_RATING_METH_065' code_map\n",
      "âœ… Updated 'STRUCTURAL_EVAL_067' code_map\n",
      "âœ… Updated 'DECK_GEOMETRY_EVAL_068' code_map\n",
      "âœ… Updated 'UNDCLRENCE_EVAL_069' code_map\n",
      "âœ… Updated 'POSTING_EVAL_070' code_map\n",
      "âœ… Updated 'WATERWAY_EVAL_071' code_map\n",
      "âœ… Updated 'APPR_ROAD_EVAL_072' code_map\n",
      "âœ… Updated 'WORK_PROPOSED_075A' code_map\n",
      "âœ… Updated 'FRACTURE_092A' code_map\n",
      "âœ… Updated 'UNDWATER_LOOK_SEE_092B' code_map\n",
      "âœ… Updated 'SPEC_INSPECT_092C' code_map\n",
      "âœ… Updated 'STRAHNET_HIGHWAY_100' code_map\n",
      "âœ… Updated 'PARALLEL_STRUCTURE_101' code_map\n",
      "âœ… Updated 'TRAFFIC_DIRECTION_102' code_map\n",
      "âœ… Updated 'HIGHWAY_SYSTEM_104' code_map\n",
      "âœ… Updated 'FEDERAL_LANDS_105' code_map\n",
      "âœ… Updated 'DECK_STRUCTURE_TYPE_107' code_map\n",
      "âœ… Updated 'SURFACE_TYPE_108A' code_map\n",
      "âœ… Updated 'MEMBRANE_TYPE_108B' code_map\n",
      "âœ… Updated 'DECK_PROTECTION_108C' code_map\n",
      "âœ… Updated 'NATIONAL_NETWORK_110' code_map\n",
      "âœ… Updated 'PIER_PROTECTION_111' code_map\n",
      "âœ… Updated 'SCOUR_CRITICAL_113' code_map\n",
      "âœ… Updated 'BRIDGE_CONDITION' code_map\n",
      "âœ… Updated 'LOWEST_RATING' code_map\n",
      "\n",
      "============================================================\n",
      "âœ… Updated 57 nominal code_maps\n",
      "ğŸ’¾ Saved to: C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\\nbi_nominal_schema_master.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "update_nominal_codemaps(\n",
    "    current_schema_path=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\\nbi_nominal_schema_master.json\",\n",
    "    old_schema_path=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\phase_1_schemas\\nbi_nominal_schema.json\",\n",
    "    output_schema_path=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\\nbi_nominal_schema_master.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852f523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual edits\n",
    "\"\"\"\n",
    "\"NFHL_FLD_ZONE\": {\n",
    "    \"type\": \"nominal\",\n",
    "    \"description\": \"The flood hazard zone classification describes the level and type of flood risk assigned to the location based on FEMA floodplain mapping.\",\n",
    "    \"code_map\": {\n",
    "      \"A\": \"high risk no elevation data\",\n",
    "      \"AE\": \"high risk with elevation data\",\n",
    "      \"AO\": \"shallow flooding sheet flow\",\n",
    "      \"AH\": \"shallow flooding with elevation\",\n",
    "      \"V\": \"coastal high velocity zone\",\n",
    "      \"VE\": \"coastal velocity with elevation\",\n",
    "      \"X\": \"minimal flood risk\",\n",
    "      \"X (0.2%)\": \"moderate risk low probability\",\n",
    "      \"D\": \"undetermined flood hazard\"\n",
    "    },\n",
    "    \"title\": \"Flood Hazard Zone\"\n",
    "  }\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "  \"NFHL_SFHA\": {\n",
    "    \"type\": \"nominal\",\n",
    "    \"description\": \"The indicator of Special Flood Hazard Area status describes whether the location is subject to a 1-percent-annual-chance (100-year) flood hazard.\",\n",
    "    \"code_map\": {\n",
    "      \"T\": \"within special flood hazard area\",\n",
    "      \"F\": \"outside special flood hazard area\"\n",
    "    },\n",
    "    \"title\": \"Flood Hazard Status\"\n",
    "  },\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "  \"SDCS\": {\n",
    "    \"type\": \"nominal\",\n",
    "    \"description\": \"The seismic design category that classifies overall seismic risk at the site and governs required detailing, analysis rigor, and design provisions.\",\n",
    "    \"code_map\": {\n",
    "      \"A\": \"very low seismic hazard\",\n",
    "      \"B\": \"low seismic hazard\",\n",
    "      \"C\": \"moderate seismic hazard\",\n",
    "      \"D\": \"high seismic hazard\",\n",
    "      \"E\": \"very high seismic hazard\",\n",
    "      \"F\": \"extreme site specific hazard\"\n",
    "    },\n",
    "    \"title\": \"Seismic Design Category\"\n",
    "  },\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73004b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                 TEST MODE - Random Snappy Paragraph Generation                 \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ğŸ“Š Total rows available: 4,914\n",
      "ğŸ² Selected random row index: 386\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Generating snappy paragraph...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… Saved to: logs\\test_paragraph_snappy_0004328B.txt\n",
      "\n",
      "ğŸ“ Paragraph length: 4,049 characters\n",
      "ğŸ“ Word count: 373 words\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Preview (first 500 characters):\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "design_peak_ground_acceleration: high seismic demand. spectral_acceleration_1.0s: moderate seismic demand. design_spectral_acceleration: high seismic demand. seismic_design_category: high seismic hazard. ground_motion_factor: minimal seismic impact. geologic_unit_name: alluvium. rock_material_type: major:{silt,sand}, minor:{gravel}, incidental:{peat, clay}. minimum_geologic_age: recently formed. geologic_characteristics: mostly unconsolidated silt, sand, and gravel valley fill with some clay; in...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_random_paragraph_snappy(schema_folder: str, data_folder: str, output_folder: str = \"logs\"):\n",
    "    \"\"\"\n",
    "    Generate a snappy paragraph for a random bridge and save to log file.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    print_section(\"TEST MODE - Random Snappy Paragraph Generation\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Get row count from first dataframe\n",
    "    num_rows = len(pairs[0][1])\n",
    "    print(f\"\\nğŸ“Š Total rows available: {num_rows:,}\")\n",
    "    \n",
    "    # Pick random row\n",
    "    random_row = random.randint(0, num_rows - 1)\n",
    "    print(f\"ğŸ² Selected random row index: {random_row}\")\n",
    "    \n",
    "    # Generate paragraph\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Generating snappy paragraph...\")\n",
    "    print(f\"{'â”€' * 80}\\n\")\n",
    "    \n",
    "    structure_id, coordinates, paragraph = generate_paragraph_snappy(random_row, pairs)\n",
    "    \n",
    "    # Save to log\n",
    "    log_file = output_folder / f\"test_paragraph_snappy_{structure_id}.txt\"\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STRUCTURE_ID: {structure_id}\\n\")\n",
    "        f.write(f\"COORDINATES: {coordinates}\\n\")\n",
    "        f.write(f\"ROW_INDEX: {random_row}\\n\")\n",
    "        f.write(f\"\\n{'=' * 80}\\n\")\n",
    "        f.write(f\"SNAPPY PARAGRAPH:\\n\")\n",
    "        f.write(f\"{'=' * 80}\\n\\n\")\n",
    "        f.write(paragraph)\n",
    "    \n",
    "    print(f\"âœ… Saved to: {log_file}\")\n",
    "    print(f\"\\nğŸ“ Paragraph length: {len(paragraph):,} characters\")\n",
    "    print(f\"ğŸ“ Word count: {len(paragraph.split()):,} words\")\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Preview (first 500 characters):\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(paragraph[:500] + \"...\\n\")\n",
    "\n",
    "# Test mode - generate one random snappy paragraph\n",
    "test_random_paragraph_snappy(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\nlp_processing\\schema_for_short\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data_fixed\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
