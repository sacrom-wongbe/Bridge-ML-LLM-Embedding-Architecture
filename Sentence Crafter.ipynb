{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5746e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import random\n",
    "\n",
    "def print_section(title, char=\"=\", width=80):\n",
    "    \"\"\"Print formatted section header.\"\"\"\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(title.center(width))\n",
    "    print(f\"{char * width}\\n\")\n",
    "\n",
    "def find_column_index(df: pd.DataFrame, column_name: str) -> int:\n",
    "    \"\"\"Find the index of a column by name.\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df.columns.get_loc(column_name)\n",
    "    raise ValueError(f\"Column '{column_name}' not found in dataframe\")\n",
    "\n",
    "def get_numerical_bin_sentence(value: float, edges: List, sentences: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Find which bin the value falls into and return corresponding sentence.\n",
    "    edges = [0, 20, 50, 80, \"inf\"] creates 4 bins:\n",
    "      - bin 0: [0, 20)\n",
    "      - bin 1: [20, 50)\n",
    "      - bin 2: [50, 80)\n",
    "      - bin 3: [80, inf)\n",
    "    \"\"\"\n",
    "    # Convert \"inf\" strings to float('inf')\n",
    "    edges_numeric = []\n",
    "    for e in edges:\n",
    "        if isinstance(e, str) and e.lower() in ['inf', '-inf']:\n",
    "            edges_numeric.append(float(e))\n",
    "        else:\n",
    "            edges_numeric.append(float(e))\n",
    "    \n",
    "    # Find bin index\n",
    "    bin_index = 0\n",
    "    for i in range(len(edges_numeric) - 1):\n",
    "        if value >= edges_numeric[i] and value < edges_numeric[i + 1]:\n",
    "            bin_index = i\n",
    "            break\n",
    "        elif i == len(edges_numeric) - 2:  # Last bin (to infinity)\n",
    "            bin_index = i\n",
    "    \n",
    "    return sentences[bin_index]\n",
    "\n",
    "def ensure_period(text: str) -> str:\n",
    "    \"\"\"Ensure text ends with a period.\"\"\"\n",
    "    text = text.strip()\n",
    "    if text and not text.endswith('.'):\n",
    "        text += '.'\n",
    "    return text\n",
    "\n",
    "def generate_sentence(column_name: str, value: Any, schema_field: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full sentence for a column value based on schema rules.\n",
    "    Returns: \"{title}: {sentence}\"\n",
    "    \"\"\"\n",
    "    # Skip null/missing values\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    \n",
    "    title = schema_field.get('title', column_name)\n",
    "    field_type = schema_field.get('type')\n",
    "    \n",
    "    if field_type == 'nominal':\n",
    "        # Match value to code_map\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        if sentence:\n",
    "            return f\"{title}: {ensure_period(sentence)}\"\n",
    "    \n",
    "    elif field_type == 'nl':\n",
    "        # Use raw value as sentence\n",
    "        return f\"{title}: {ensure_period(str(value))}\"\n",
    "    \n",
    "    elif field_type == 'numerical':\n",
    "        # Use semantic bins\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        sentences = semantic_bins.get('sentences', [])\n",
    "        \n",
    "        if edges and sentences:\n",
    "            try:\n",
    "                sentence = get_numerical_bin_sentence(float(value), edges, sentences)\n",
    "                return f\"{title}: {ensure_period(sentence)}\"\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    elif field_type == 'numerical_coded':\n",
    "        # Try code_map first, fallback to semantic bins\n",
    "        code_map = schema_field.get('code_map') or {}\n",
    "        sentence = code_map.get(str(value))\n",
    "        \n",
    "        if sentence:\n",
    "            return f\"{title}: {ensure_period(sentence)}\"\n",
    "        \n",
    "        # Fallback to semantic bins\n",
    "        semantic_bins = schema_field.get('semantic_bins', {})\n",
    "        edges = semantic_bins.get('edges', [])\n",
    "        sentences = semantic_bins.get('sentences', [])\n",
    "        \n",
    "        if edges and sentences:\n",
    "            try:\n",
    "                sentence = get_numerical_bin_sentence(float(value), edges, sentences)\n",
    "                return f\"{title}: {ensure_period(sentence)}\"\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"    âš ï¸  Error processing numerical_coded value {value}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_schema_csv_pairs(schema_folder: Path, data_folder: Path) -> List[Tuple[Dict, pd.DataFrame, str]]:\n",
    "    \"\"\"\n",
    "    Load matching schema/CSV pairs.\n",
    "    Returns: List of (schema_dict, dataframe, source_name) tuples\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Get all schema files\n",
    "    schema_files = sorted(schema_folder.glob(\"*_schema_master.json\"))\n",
    "    \n",
    "    for schema_file in schema_files:\n",
    "        # Derive CSV filename\n",
    "        csv_name = schema_file.stem.replace(\"_schema_master\", \"\") + \".csv\"\n",
    "        csv_path = data_folder / csv_name\n",
    "        \n",
    "        if not csv_path.exists():\n",
    "            print(f\"âš ï¸  Warning: No matching CSV for {schema_file.name}\")\n",
    "            continue\n",
    "        \n",
    "        # Load schema\n",
    "        with open(schema_file, 'r', encoding='utf-8') as f:\n",
    "            schema = json.load(f)\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        source_name = csv_name.replace(\".csv\", \"\")\n",
    "        pairs.append((schema, df, source_name))\n",
    "        print(f\"âœ… Loaded: {schema_file.name} + {csv_name}\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_paragraph_for_row(row_index: int, schema_csv_pairs: List[Tuple[Dict, pd.DataFrame, str]]) -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Generate a paragraph for a single row across all CSVs.\n",
    "    Returns: (structure_id, coordinates, paragraph)\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    structure_id = None\n",
    "    coordinates = None\n",
    "    \n",
    "    for schema, df, source_name in schema_csv_pairs:\n",
    "        row = df.iloc[row_index]\n",
    "        \n",
    "        # Extract structure_id and coordinates from first pair\n",
    "        if structure_id is None:\n",
    "            structure_id = row.iloc[0] if 'STRUCTURE_ID' not in df.columns else row['STRUCTURE_ID']\n",
    "        if coordinates is None:\n",
    "            coordinates = row.iloc[1] if 'COORDINATES' not in df.columns else row['COORDINATES']\n",
    "        \n",
    "        # Process each column (skip STRUCTURE_ID and COORDINATES)\n",
    "        for column_name, schema_field in schema.items():\n",
    "            if column_name in ['STRUCTURE_ID', 'COORDINATES']:\n",
    "                continue\n",
    "            \n",
    "            if column_name not in df.columns:\n",
    "                print(f\"    âš ï¸  Column mismatch: '{column_name}' in schema but not in {source_name}.csv\")\n",
    "                raise ValueError(f\"Column mismatch: {column_name}\")\n",
    "            \n",
    "            value = row[column_name]\n",
    "            sentence = generate_sentence(column_name, value, schema_field)\n",
    "            \n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "    \n",
    "    # Combine all sentences into one paragraph\n",
    "    paragraph = \" \".join(sentences)\n",
    "    \n",
    "    return str(structure_id), str(coordinates), paragraph\n",
    "\n",
    "def test_random_paragraph(schema_folder: str, data_folder: str, output_folder: str = \"logs\"):\n",
    "    \"\"\"\n",
    "    Generate a paragraph for a random bridge and save to log file.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    \n",
    "    print_section(\"TEST MODE - Random Paragraph Generation\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Get row count from first dataframe\n",
    "    num_rows = len(pairs[0][1])\n",
    "    print(f\"\\nðŸ“Š Total rows available: {num_rows:,}\")\n",
    "    \n",
    "    # Pick random row\n",
    "    random_row = random.randint(0, num_rows - 1)\n",
    "    print(f\"ðŸŽ² Selected random row index: {random_row}\")\n",
    "    \n",
    "    # Generate paragraph\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Generating paragraph...\")\n",
    "    print(f\"{'â”€' * 80}\\n\")\n",
    "    \n",
    "    structure_id, coordinates, paragraph = generate_paragraph_for_row(random_row, pairs)\n",
    "    \n",
    "    # Save to log\n",
    "    log_file = output_folder / f\"test_paragraph_{structure_id}.txt\"\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STRUCTURE_ID: {structure_id}\\n\")\n",
    "        f.write(f\"COORDINATES: {coordinates}\\n\")\n",
    "        f.write(f\"ROW_INDEX: {random_row}\\n\")\n",
    "        f.write(f\"\\n{'=' * 80}\\n\")\n",
    "        f.write(f\"PARAGRAPH:\\n\")\n",
    "        f.write(f\"{'=' * 80}\\n\\n\")\n",
    "        f.write(paragraph)\n",
    "    \n",
    "    print(f\"âœ… Saved to: {log_file}\")\n",
    "    print(f\"\\nðŸ“ Paragraph length: {len(paragraph):,} characters\")\n",
    "    print(f\"ðŸ“ Word count: {len(paragraph.split()):,} words\")\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"Preview (first 500 characters):\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(paragraph[:500] + \"...\\n\")\n",
    "\n",
    "def generate_all_paragraphs(schema_folder: str, data_folder: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Generate paragraphs for all bridges and save to CSV.\n",
    "    \"\"\"\n",
    "    schema_folder = Path(schema_folder)\n",
    "    data_folder = Path(data_folder)\n",
    "    output_path = Path(output_file)\n",
    "    \n",
    "    print_section(\"GENERATING PARAGRAPHS FOR ALL BRIDGES\")\n",
    "    \n",
    "    # Load all pairs\n",
    "    print(\"Loading schema/CSV pairs...\")\n",
    "    pairs = load_schema_csv_pairs(schema_folder, data_folder)\n",
    "    \n",
    "    if not pairs:\n",
    "        print(\"âŒ No valid schema/CSV pairs found\")\n",
    "        return\n",
    "    \n",
    "    # Validate row counts match\n",
    "    print(\"\\nðŸ“Š Validating row counts...\")\n",
    "    row_counts = [len(df) for _, df, _ in pairs]\n",
    "    if len(set(row_counts)) > 1:\n",
    "        print(\"âŒ ERROR: Row count mismatch across CSVs!\")\n",
    "        for schema, df, name in pairs:\n",
    "            print(f\"   {name}: {len(df):,} rows\")\n",
    "        raise ValueError(\"Row count mismatch\")\n",
    "    \n",
    "    num_rows = row_counts[0]\n",
    "    print(f\"âœ… All CSVs have {num_rows:,} rows\")\n",
    "    \n",
    "    # Generate paragraphs\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Generating paragraphs...\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for row_idx in range(num_rows):\n",
    "        if row_idx % 100 == 0:\n",
    "            print(f\"Progress: {row_idx:,} / {num_rows:,} ({100 * row_idx / num_rows:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            structure_id, coordinates, paragraph = generate_paragraph_for_row(row_idx, pairs)\n",
    "            results.append({\n",
    "                'STRUCTURE_ID': structure_id,\n",
    "                'COORDINATES': coordinates,\n",
    "                'PARAGRAPH': paragraph\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error at row {row_idx}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"\\nðŸ’¾ Saving to {output_path}...\")\n",
    "    df_output = pd.DataFrame(results)\n",
    "    df_output.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print_section(\"COMPLETE\")\n",
    "    print(f\"âœ… Generated {len(results):,} paragraphs\")\n",
    "    print(f\"ðŸ’¾ Saved to: {output_path}\")\n",
    "    print(f\"\\nðŸ“ Statistics:\")\n",
    "    print(f\"   Average paragraph length: {df_output['PARAGRAPH'].str.len().mean():.0f} characters\")\n",
    "    print(f\"   Max paragraph length: {df_output['PARAGRAPH'].str.len().max():,} characters\")\n",
    "    print(f\"   Min paragraph length: {df_output['PARAGRAPH'].str.len().min():,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35778998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    TEST MODE - Random Paragraph Generation                     \n",
      "================================================================================\n",
      "\n",
      "Loading schema/CSV pairs...\n",
      "âœ… Loaded: design_maps_schema_master.json + design_maps.csv\n",
      "âœ… Loaded: macrostrat_schema_master.json + macrostrat.csv\n",
      "âœ… Loaded: nbi_nominal_schema_master.json + nbi_nominal.csv\n",
      "âœ… Loaded: nbi_numerical_coded_schema_master.json + nbi_numerical_coded.csv\n",
      "âœ… Loaded: nbi_numerical_schema_master.json + nbi_numerical.csv\n",
      "âœ… Loaded: nfhl_fema_flood_schema_master.json + nfhl_fema_flood.csv\n",
      "âœ… Loaded: nshm_hazard_grid_schema_master.json + nshm_hazard_grid.csv\n",
      "\n",
      "ðŸ“Š Total rows available: 4,914\n",
      "ðŸŽ² Selected random row index: 1192\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Generating paragraph...\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test mode - generate one random paragraph\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtest_random_paragraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mwongb\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBridge-ML\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBridge-ML-LLM-Embedding-Architecture\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mfinal_schemas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mwongb\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBridge-ML\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBridge-ML-LLM-Embedding-Architecture\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43menriched_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 217\u001b[39m, in \u001b[36mtest_random_paragraph\u001b[39m\u001b[34m(schema_folder, data_folder, output_folder)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating paragraph...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mâ”€\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m structure_id, coordinates, paragraph = \u001b[43mgenerate_paragraph_for_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Save to log\u001b[39;00m\n\u001b[32m    220\u001b[39m log_file = output_folder / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest_paragraph_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstructure_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mgenerate_paragraph_for_row\u001b[39m\u001b[34m(row_index, schema_csv_pairs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn mismatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m value = row[column_name]\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m sentence = \u001b[43mgenerate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sentence:\n\u001b[32m    178\u001b[39m     sentences.append(sentence)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mgenerate_sentence\u001b[39m\u001b[34m(column_name, value, schema_field)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m field_type == \u001b[33m'\u001b[39m\u001b[33mnumerical_coded\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# Try code_map first, fallback to semantic bins\u001b[39;00m\n\u001b[32m     94\u001b[39m     code_map = schema_field.get(\u001b[33m'\u001b[39m\u001b[33mcode_map\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     sentence = \u001b[43mcode_map\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[38;5;28mstr\u001b[39m(value))\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sentence:\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mensure_period(sentence)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Test mode - generate one random paragraph\n",
    "test_random_paragraph(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full generation\n",
    "generate_all_paragraphs(\n",
    "    schema_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\final_schemas\",\n",
    "    data_folder=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\",\n",
    "    output_file=r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\bridge_paragraphs.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
