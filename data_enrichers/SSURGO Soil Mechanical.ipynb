{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76242e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyproj\n",
      "  Downloading pyproj-3.7.2-cp313-cp313-win_amd64.whl.metadata (31 kB)\n",
      "Collecting shapely\n",
      "  Downloading shapely-2.1.2-cp313-cp313-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting fiona\n",
      "  Downloading fiona-1.10.1-cp313-cp313-win_amd64.whl.metadata (58 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (2.3.3)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.12.1-cp313-cp313-win_amd64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from pyproj) (2025.10.5)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from fiona) (25.4.0)\n",
      "Collecting click~=8.0 (from fiona)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click-plugins>=1.0 (from fiona)\n",
      "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cligj>=0.5 (from fiona)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from click~=8.0->fiona) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wongb\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
      "Downloading pyproj-3.7.2-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.3 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 14.1 MB/s  0:00:00\n",
      "Downloading shapely-2.1.2-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.6/1.7 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 9.8 MB/s  0:00:00\n",
      "Downloading fiona-1.10.1-cp313-cp313-win_amd64.whl (24.5 MB)\n",
      "   ---------------------------------------- 0.0/24.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/24.5 MB 13.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.7/24.5 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.4/24.5 MB 15.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.7/24.5 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.9/24.5 MB 14.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.0/24.5 MB 14.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.5 MB 15.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.5 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.5/24.5 MB 14.6 MB/s  0:00:01\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading pyogrio-0.12.1-cp313-cp313-win_amd64.whl (22.9 MB)\n",
      "   ---------------------------------------- 0.0/22.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/22.9 MB 8.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 4.7/22.9 MB 12.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.1/22.9 MB 13.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.0/22.9 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.2/22.9 MB 14.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 17.6/22.9 MB 14.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 20.2/22.9 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 22.9/22.9 MB 14.3 MB/s  0:00:01\n",
      "Installing collected packages: shapely, pyproj, pyogrio, click, cligj, click-plugins, geopandas, fiona\n",
      "\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ---------------------------------------- 0/8 [shapely]\n",
      "   ----- ---------------------------------- 1/8 [pyproj]\n",
      "   ----- ---------------------------------- 1/8 [pyproj]\n",
      "   ---------- ----------------------------- 2/8 [pyogrio]\n",
      "   ---------- ----------------------------- 2/8 [pyogrio]\n",
      "   ---------- ----------------------------- 2/8 [pyogrio]\n",
      "   ---------- ----------------------------- 2/8 [pyogrio]\n",
      "   --------------- ------------------------ 3/8 [click]\n",
      "   ------------------------- -------------- 5/8 [click-plugins]\n",
      "   ------------------------------ --------- 6/8 [geopandas]\n",
      "   ------------------------------ --------- 6/8 [geopandas]\n",
      "   ------------------------------ --------- 6/8 [geopandas]\n",
      "   ------------------------------ --------- 6/8 [geopandas]\n",
      "   ------------------------------ --------- 6/8 [geopandas]\n",
      "   ------------------------------ --------- 6/8 [geopandas]\n",
      "   ----------------------------------- ---- 7/8 [fiona]\n",
      "   ----------------------------------- ---- 7/8 [fiona]\n",
      "   ----------------------------------- ---- 7/8 [fiona]\n",
      "   ----------------------------------- ---- 7/8 [fiona]\n",
      "   ----------------------------------- ---- 7/8 [fiona]\n",
      "   ---------------------------------------- 8/8 [fiona]\n",
      "\n",
      "Successfully installed click-8.3.1 click-plugins-1.1.1.2 cligj-0.7.2 fiona-1.10.1 geopandas-1.1.1 pyogrio-0.12.1 pyproj-3.7.2 shapely-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install geopandas pyproj shapely fiona numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539bd150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Loading structure coordinates...\n",
      "  ✓ Loaded 4914 structures in 0.19s\n",
      "\n",
      "[STEP 2] Loading gSSURGO polygons and tables...\n",
      "  Loading MUPOLYGON (this may take a moment with complex geometries)...\n",
      "  ✗ Error loading MUPOLYGON: \n",
      "  ✗ Error loading MUPOLYGON: \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Loading MUPOLYGON (this may take a moment with complex geometries)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     gdf_mu = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGSSURGO_GDB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMUPOLYGON_LAYER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✓ Loaded MUPOLYGON layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(gdf_mu)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m polygons in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt2\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# AGGRESSIVE simplification to prevent bad_alloc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\geopandas\\io\\file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\geopandas\\io\\file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyogrio\\geopandas.py:382\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, datetime_as_string, mixed_offsets_as_utc, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m gdal_force_2d = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m use_arrow \u001b[38;5;28;01melse\u001b[39;00m force_2d\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# Always read datetimes as string values to preserve (mixed) time zone info\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# correctly. If arrow is not used, it is needed because numpy does not\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# directly support time zones + performance is also a lot better. If arrow\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# is used, needed because datetime columns don't support mixed time zone\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;66;03m# offsets + e.g. for .fgb files time zone info isn't handled correctly even\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[38;5;66;03m# for unique time zone offsets if datetimes are not read as string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pyogrio\\raw.py:200\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    196\u001b[39m \n\u001b[32m    197\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    198\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1503\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1159\u001b[39m, in \u001b[36mpyogrio._io.get_features\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:899\u001b[39m, in \u001b[36mpyogrio._io.process_geometry\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SET GDAL/OGRIO OPTIONS TO SKIP COMPLEX POLYGON PROCESSING\n",
    "# This prevents the \"organizePolygons\" warning and massive slowdowns\n",
    "os.environ['GDAL_DISABLE_READDIR_ON_OPEN'] = 'YES'\n",
    "os.environ['GDAL_INGESTED_FILE_CACHE_SIZE'] = '0'\n",
    "\n",
    "# Import ogrio after setting env vars\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='organizePolygons')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# USER PATHS\n",
    "# ---------------------------------------------------------\n",
    "GSSURGO_GDB = r\"C:\\Users\\wongb\\Downloads\\gSSURGO_CONUS\\gSSURGO_CONUS.gdb\"  # <-- change this\n",
    "STRUCT_COORDS_CSV = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\\structure_coordinates.csv\"\n",
    "OUT_CSV = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\enriched_data\\gssurgo_enriched.csv\"\n",
    "\n",
    "# Crash prevention settings\n",
    "CHUNK_SIZE = 500  # Process structures in chunks to avoid memory spikes during spatial join\n",
    "ENABLE_CHECKPOINTS = True  # Save intermediate results\n",
    "CHECKPOINT_DIR = r\"C:\\Users\\wongb\\Bridge-ML\\Bridge-ML-LLM-Embedding-Architecture\\checkpoints\"\n",
    "\n",
    "# Create checkpoint directory if needed\n",
    "if ENABLE_CHECKPOINTS and not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "    print(f\"Created checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# If you're not sure about the polygon layer name, uncomment this to list all layers:\n",
    "# from fiona import listlayers\n",
    "# print(listlayers(GSSURGO_GDB))\n",
    "\n",
    "# Typical layer names:\n",
    "MUPOLYGON_LAYER = \"MUPOLYGON\"   # or \"MapunitPoly\" etc.\n",
    "MUAGGATT_LAYER = \"muaggatt\"\n",
    "COMPONENT_LAYER = \"component\"\n",
    "CHORIZON_LAYER = \"chorizon\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load structure coordinates as a GeoDataFrame\n",
    "# ---------------------------------------------------------\n",
    "t1 = time.time()\n",
    "print(\"\\n[STEP 1] Loading structure coordinates...\")\n",
    "\n",
    "df_struct = pd.read_csv(STRUCT_COORDS_CSV)\n",
    "\n",
    "def parse_coord(x):\n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except Exception:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "df_struct[\"COORDINATES\"] = df_struct[\"COORDINATES\"].apply(parse_coord)\n",
    "df_struct[\"LAT\"] = df_struct[\"COORDINATES\"].apply(lambda c: c[0])\n",
    "df_struct[\"LON\"] = df_struct[\"COORDINATES\"].apply(lambda c: c[1])\n",
    "\n",
    "gdf_struct = gpd.GeoDataFrame(\n",
    "    df_struct,\n",
    "    geometry=gpd.points_from_xy(df_struct[\"LON\"], df_struct[\"LAT\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(f\"  ✓ Loaded {len(gdf_struct)} structures in {time.time() - t1:.2f}s\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Load gSSURGO polygon layer & tables\n",
    "# ---------------------------------------------------------\n",
    "t2 = time.time()\n",
    "print(\"\\n[STEP 2] Loading gSSURGO polygons and tables...\")\n",
    "\n",
    "try:\n",
    "    print(\"  Loading MUPOLYGON (this may take a moment with complex geometries)...\")\n",
    "    gdf_mu = gpd.read_file(GSSURGO_GDB, layer=MUPOLYGON_LAYER)\n",
    "    print(f\"  ✓ Loaded MUPOLYGON layer: {len(gdf_mu)} polygons in {time.time() - t2:.2f}s\")\n",
    "    \n",
    "    # AGGRESSIVE simplification to prevent bad_alloc\n",
    "    print(\"  Aggressively simplifying polygon geometries...\")\n",
    "    t_simplify = time.time()\n",
    "    gdf_mu['geometry'] = gdf_mu.geometry.simplify(tolerance=0.001, preserve_topology=True)\n",
    "    gdf_mu = gdf_mu.reset_index(drop=True)\n",
    "    gc.collect()  # Force garbage collection\n",
    "    print(f\"  ✓ Simplified geometries in {time.time() - t_simplify:.2f}s\")\n",
    "    \n",
    "    # Most gSSURGO polygon layers are in EPSG:4269 (NAD83), but we'll reproject to WGS84.\n",
    "    if gdf_mu.crs is None:\n",
    "        gdf_mu.set_crs(\"EPSG:4269\", inplace=True)\n",
    "        print(\"    (Set CRS to EPSG:4269)\")\n",
    "    \n",
    "    t_reproject = time.time()\n",
    "    gdf_mu = gdf_mu.to_crs(\"EPSG:4326\")\n",
    "    gc.collect()\n",
    "    print(f\"  ✓ Reprojected to EPSG:4326 in {time.time() - t_reproject:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error loading MUPOLYGON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Non-spatial tables (GeoPandas will load them as DataFrames with no geometry)\n",
    "try:\n",
    "    t_muagg = time.time()\n",
    "    df_muagg = gpd.read_file(GSSURGO_GDB, layer=MUAGGATT_LAYER)\n",
    "    print(f\"  ✓ Loaded MUAGGATT in {time.time() - t_muagg:.2f}s\")\n",
    "    \n",
    "    t_comp = time.time()\n",
    "    df_comp = gpd.read_file(GSSURGO_GDB, layer=COMPONENT_LAYER)\n",
    "    print(f\"  ✓ Loaded COMPONENT in {time.time() - t_comp:.2f}s\")\n",
    "    \n",
    "    t_ch = time.time()\n",
    "    df_ch = gpd.read_file(GSSURGO_GDB, layer=CHORIZON_LAYER)\n",
    "    print(f\"  ✓ Loaded CHORIZON in {time.time() - t_ch:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Error loading attribute tables: {e}\")\n",
    "    raise\n",
    "\n",
    "# Make sure they're plain pandas DataFrames (drop geometry if present)\n",
    "df_muagg = pd.DataFrame(df_muagg.drop(columns=[c for c in df_muagg.columns if c == \"geometry\"]))\n",
    "df_comp = pd.DataFrame(df_comp.drop(columns=[c for c in df_comp.columns if c == \"geometry\"]))\n",
    "df_ch = pd.DataFrame(df_ch.drop(columns=[c for c in df_ch.columns if c == \"geometry\"]))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Spatial join: bridge point → MUKEY (CHUNKED to prevent memory overflow)\n",
    "# ---------------------------------------------------------\n",
    "t3 = time.time()\n",
    "print(\"\\n[STEP 3] Performing spatial join (bridges → MUKEY) in chunks...\")\n",
    "print(f\"  Processing {len(gdf_struct)} structures in chunks of {CHUNK_SIZE}...\")\n",
    "\n",
    "# Process structures in chunks to avoid memory explosion\n",
    "chunk_results = []\n",
    "num_chunks = (len(gdf_struct) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "for chunk_idx in tqdm(range(num_chunks), desc=\"Spatial join chunks\", unit=\" chunk\"):\n",
    "    start_idx = chunk_idx * CHUNK_SIZE\n",
    "    end_idx = min(start_idx + CHUNK_SIZE, len(gdf_struct))\n",
    "    \n",
    "    gdf_struct_chunk = gdf_struct.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    try:\n",
    "        # Spatial join for this chunk\n",
    "        chunk_joined = gpd.sjoin(\n",
    "            gdf_struct_chunk,\n",
    "            gdf_mu[[\"MUKEY\", \"geometry\"]],\n",
    "            how=\"left\",\n",
    "            predicate=\"intersects\"\n",
    "        ).drop(columns=[\"index_right\"], errors='ignore')\n",
    "        \n",
    "        chunk_results.append(chunk_joined)\n",
    "        del chunk_joined  # Free memory\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ✗ Error in chunk {chunk_idx}: {e}\")\n",
    "        print(f\"  Retrying chunk {chunk_idx} with 'within' predicate...\")\n",
    "        try:\n",
    "            chunk_joined = gpd.sjoin(\n",
    "                gdf_struct_chunk,\n",
    "                gdf_mu[[\"MUKEY\", \"geometry\"]],\n",
    "                how=\"left\",\n",
    "                predicate=\"within\"\n",
    "            ).drop(columns=[\"index_right\"], errors='ignore')\n",
    "            chunk_results.append(chunk_joined)\n",
    "            del chunk_joined\n",
    "            gc.collect()\n",
    "        except Exception as e2:\n",
    "            print(f\"  ✗ Both predicates failed for chunk {chunk_idx}: {e2}\")\n",
    "            raise\n",
    "\n",
    "# Concatenate all chunks\n",
    "gdf_struct_mu = pd.concat(chunk_results, ignore_index=True)\n",
    "del chunk_results\n",
    "gc.collect()\n",
    "print(f\"\\n  ✓ Spatial join completed in {time.time() - t3:.2f}s ({len(gdf_struct_mu)} records matched)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Prepare muaggatt: mapunit-level aggregated features\n",
    "# ---------------------------------------------------------\n",
    "t4 = time.time()\n",
    "print(\"\\n[STEP 4] Preparing muaggatt features...\")\n",
    "\n",
    "mu_cols = [\n",
    "    \"MUKEY\",\n",
    "    \"brockdepmin\",   # min depth to bedrock\n",
    "    \"wtdepannmin\",   # min annual water table depth\n",
    "    \"wtdepaprjunmin\",\n",
    "    \"slopegraddcp\",  # slope gradient, dominant\n",
    "    \"drclassdcd\",    # drainage class, dominant\n",
    "    \"hydgrpdcd\",     # hydrologic group, dominant\n",
    "    \"flodfreqdcd\",   # flooding frequency, dominant\n",
    "    \"aws0100wta\",    # available water storage 0-100 cm\n",
    "    \"aws0150wta\"     # available water storage 0-150 cm\n",
    "]\n",
    "\n",
    "df_muagg_sub = df_muagg[[c for c in mu_cols if c in df_muagg.columns]].copy()\n",
    "df_muagg_sub = df_muagg_sub.add_prefix(\"MA_\")\n",
    "\n",
    "# MUKEY got prefixed, rename back that one:\n",
    "df_muagg_sub = df_muagg_sub.rename(columns={\"MA_MUKEY\": \"MUKEY\"})\n",
    "print(f\"  ✓ Prepared {len(df_muagg_sub.columns)} muaggatt features in {time.time() - t4:.2f}s\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Prepare component table: dominant component per MUKEY\n",
    "# ---------------------------------------------------------\n",
    "t5 = time.time()\n",
    "print(\"\\n[STEP 5] Preparing component-level features (dominant component per mapunit)...\")\n",
    "\n",
    "comp_cols = [\n",
    "    \"MUKEY\",\n",
    "    \"COKEY\",\n",
    "    \"comppct_r\",   # component percent\n",
    "    \"slope_r\",\n",
    "    \"drainagecl\",\n",
    "    \"hydricrating\",\n",
    "    \"hydgrp\",\n",
    "    \"corcon\",\n",
    "    \"corsteel\",\n",
    "    \"soilslippot\",\n",
    "    \"frostact\",\n",
    "    \"elev_r\"\n",
    "]\n",
    "df_comp_sub = df_comp[[c for c in comp_cols if c in df_comp.columns]].copy()\n",
    "\n",
    "# Choose dominant component per MUKEY by highest comppct_r\n",
    "df_comp_sub = df_comp_sub.sort_values([\"MUKEY\", \"comppct_r\"], ascending=[True, False])\n",
    "df_comp_dom = df_comp_sub.groupby(\"MUKEY\", as_index=False).first()\n",
    "\n",
    "df_comp_dom = df_comp_dom.add_prefix(\"CO_\")\n",
    "df_comp_dom = df_comp_dom.rename(columns={\"CO_MUKEY\": \"MUKEY\"})  # restore MUKEY\n",
    "print(f\"  ✓ Prepared {len(df_comp_dom)} dominant components in {time.time() - t5:.2f}s\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Prepare chorizon: depth-weighted averages over top 100 cm\n",
    "# ---------------------------------------------------------\n",
    "t6 = time.time()\n",
    "print(\"\\n[STEP 6] Preparing horizon-level aggregated features (0–100 cm)...\")\n",
    "\n",
    "ch_cols = [\n",
    "    \"COKEY\",\n",
    "    \"hzdept_r\",    # horizon top depth (cm)\n",
    "    \"hzdepb_r\",    # horizon bottom depth (cm)\n",
    "    \"sandtotal_r\",\n",
    "    \"silttotal_r\",\n",
    "    \"claytotal_r\",\n",
    "    \"dbovendry_r\",\n",
    "    \"ksat_r\",\n",
    "    \"awc_r\",\n",
    "    \"om_r\",\n",
    "    \"ll_r\",\n",
    "    \"pi_r\",\n",
    "    \"wsatiated_r\"\n",
    "]\n",
    "df_ch_sub = df_ch[[c for c in ch_cols if c in df_ch.columns]].copy()\n",
    "\n",
    "# Compute horizon thickness and clip to [0, 100] cm for depth-weighted averages\n",
    "df_ch_sub[\"hzthk\"] = df_ch_sub[\"hzdepb_r\"] - df_ch_sub[\"hzdept_r\"]\n",
    "\n",
    "# Clip horizons to 0–100cm.\n",
    "top, bottom = 0.0, 100.0\n",
    "df_ch_sub[\"eff_thk\"] = (\n",
    "    np.minimum(df_ch_sub[\"hzdepb_r\"], bottom) - np.maximum(df_ch_sub[\"hzdept_r\"], top)\n",
    ").clip(lower=0)\n",
    "\n",
    "# Keep only horizons that intersect 0–100cm and have positive thickness\n",
    "df_ch_sub = df_ch_sub[df_ch_sub[\"eff_thk\"] > 0].copy()\n",
    "print(f\"  ✓ Filtered to {len(df_ch_sub)} horizons (0-100cm) in {time.time() - t6:.2f}s\")\n",
    "\n",
    "def depth_weighted(group, col):\n",
    "    \"\"\"\n",
    "    Depth-weighted average of 'col' over eff_thk.\n",
    "    \"\"\"\n",
    "    vals = group[col].to_numpy(dtype=float)\n",
    "    w = group[\"eff_thk\"].to_numpy(dtype=float)\n",
    "    mask = ~np.isnan(vals)\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "    vals = vals[mask]\n",
    "    w = w[mask]\n",
    "    if w.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.sum(vals * w) / np.sum(w)\n",
    "\n",
    "agg_dict = {}\n",
    "for col in [\n",
    "    \"sandtotal_r\",\n",
    "    \"silttotal_r\",\n",
    "    \"claytotal_r\",\n",
    "    \"dbovendry_r\",\n",
    "    \"ksat_r\",\n",
    "    \"awc_r\",\n",
    "    \"om_r\",\n",
    "    \"ll_r\",\n",
    "    \"pi_r\",\n",
    "    \"wsatiated_r\"\n",
    "]:\n",
    "    if col in df_ch_sub.columns:\n",
    "        agg_dict[col] = lambda g, c=col: depth_weighted(g, c)\n",
    "\n",
    "t_group = time.time()\n",
    "grouped = df_ch_sub.groupby(\"COKEY\")\n",
    "print(f\"  ✓ Grouped by COKEY ({len(grouped)} groups) in {time.time() - t_group:.2f}s\")\n",
    "\n",
    "rows = []\n",
    "print(\"  Aggregating horizon features per COKEY (0–100 cm)...\")\n",
    "for cokey, g in tqdm(grouped, total=len(grouped), desc=\"  Horizon agg\", unit=\" COKEY\"):\n",
    "    row = {\"COKEY\": cokey}\n",
    "    for col, func in agg_dict.items():\n",
    "        row[col] = func(g)\n",
    "    rows.append(row)\n",
    "\n",
    "df_ch_agg = pd.DataFrame(rows)\n",
    "\n",
    "# Prefix and join with component-level dominant COKEY later\n",
    "df_ch_agg = df_ch_agg.add_prefix(\"CH_\")\n",
    "df_ch_agg = df_ch_agg.rename(columns={\"CH_COKEY\": \"CO_COKEY\"})\n",
    "print(f\"  ✓ Horizon aggregation completed in {time.time() - t6:.2f}s total for Step 6\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. Merge muaggatt + component + chorizon, then attach to bridges\n",
    "# ---------------------------------------------------------\n",
    "t7 = time.time()\n",
    "print(\"\\n[STEP 7] Merging soil tables and attaching to structures...\")\n",
    "\n",
    "# Merge muaggatt and component-dominant on MUKEY\n",
    "df_soil = gdf_struct_mu.merge(df_muagg_sub, on=\"MUKEY\", how=\"left\")\n",
    "print(f\"  ✓ Merged muaggatt in {time.time() - t7:.2f}s\")\n",
    "\n",
    "t_comp_merge = time.time()\n",
    "df_soil = df_soil.merge(df_comp_dom, on=\"MUKEY\", how=\"left\")\n",
    "print(f\"  ✓ Merged component in {time.time() - t_comp_merge:.2f}s\")\n",
    "\n",
    "# Merge chorizon aggregates on dominant component COKEY\n",
    "if \"CO_COKEY\" in df_soil.columns:\n",
    "    t_ch_merge = time.time()\n",
    "    df_soil = df_soil.merge(df_ch_agg, on=\"CO_COKEY\", how=\"left\")\n",
    "    print(f\"  ✓ Merged chorizon in {time.time() - t_ch_merge:.2f}s\")\n",
    "else:\n",
    "    print(\"  ✗ Warning: CO_COKEY not found after component merge; horizon features will be missing.\")\n",
    "\n",
    "# Drop geometry for final CSV\n",
    "df_final = pd.DataFrame(df_soil.drop(columns=[\"geometry\"]))\n",
    "\n",
    "t_save = time.time()\n",
    "df_final.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\n✓ Saved gSSURGO-enriched dataset ({len(df_final)} records) in {time.time() - t_save:.2f}s\")\n",
    "print(f\"  Location: {OUT_CSV}\")\n",
    "\n",
    "# Final timing summary\n",
    "total_time = time.time() - t1\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOTAL EXECUTION TIME: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Alternative: Query SDA over HTTPS (post.rest)\n",
    "# - Avoids downloading/handling the full gSSURGO geodatabase\n",
    "# - Returns MUKEY + selected attributes for point locations\n",
    "# ---------------------------------------------------------\n",
    "SDA_POST_URL = \"https://SDMDataAccess.sc.egov.usda.gov/Tabular/post.rest\"\n",
    "\n",
    "# Columns to return from mapunit/legend\n",
    "SDA_COLUMNS = [\n",
    "    \"mu.mukey\",\n",
    "    \"mu.musym\",\n",
    "    \"mu.muname\",\n",
    "    \"mu.brockdepmin\",\n",
    "    \"mu.wtdepannmin\",\n",
    "    \"mu.wtdepaprjunmin\",\n",
    "    \"mu.slopegraddcp\",\n",
    "    \"mu.drclassdcd\",\n",
    "    \"mu.hydgrpdcd\",\n",
    "    \"mu.flodfreqdcd\",\n",
    "    \"mu.aws0100wta\",\n",
    "    \"mu.aws0150wta\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_point_query(points: List[Tuple[float, float]]) -> str:\n",
    "    \"\"\"Build a SQL query to fetch MUKEY + mapunit attributes for (lat, lon) points via SDA macro.\n",
    "    Uses SDA_Get_Mukey_from_intersect instead of mapunitpoly table.\n",
    "    \"\"\"\n",
    "    # Number the points for a stable pid\n",
    "    vals_rows = []\n",
    "    for idx, (lat, lon) in enumerate(points):\n",
    "        vals_rows.append(\n",
    "            f\"({idx}, {lat}, {lon}, geometry::STPointFromText('POINT({lon} {lat})', 4326))\"\n",
    "        )\n",
    "    vals_clause = \",\\n    \".join(vals_rows)\n",
    "\n",
    "    cols = \",\".join(SDA_COLUMNS)\n",
    "    sql = f\"\"\"\n",
    "    WITH pts AS (\n",
    "        SELECT * FROM (VALUES\n",
    "        {vals_clause}\n",
    "        ) AS v(pid, lat, lon, geom)\n",
    "    ),\n",
    "    hits AS (\n",
    "        SELECT p.pid, p.lat, p.lon, x.mukey, x.areasymbol\n",
    "        FROM pts p\n",
    "        CROSS APPLY SDA_Get_Mukey_from_intersect(p.geom) AS x\n",
    "    )\n",
    "    SELECT {cols}, h.areasymbol, h.pid, h.lat, h.lon\n",
    "    FROM hits h\n",
    "    JOIN mapunit mu ON mu.mukey = h.mukey\n",
    "    JOIN legend l ON mu.lkey = l.lkey\n",
    "    \"\"\"\n",
    "    return sql\n",
    "\n",
    "\n",
    "def sda_query(sql: str) -> pd.DataFrame:\n",
    "    \"\"\"Execute an SDA post.rest SQL query and return a DataFrame.\"\"\"\n",
    "    payload = {\n",
    "        \"format\": \"JSON+COLUMNNAME\",  # first row = column names\n",
    "        \"query\": sql,\n",
    "    }\n",
    "    resp = requests.post(SDA_POST_URL, data=payload, timeout=120)\n",
    "    if not resp.ok:\n",
    "        # Print server message to help diagnose (often includes SQL error)\n",
    "        msg = resp.text[:2000]\n",
    "        raise RuntimeError(f\"SDA request failed: HTTP {resp.status_code}\\n{msg}\")\n",
    "\n",
    "    data = resp.json()\n",
    "    if not data:\n",
    "        return pd.DataFrame()\n",
    "    # First row = column names\n",
    "    cols = data[0]\n",
    "    rows = data[1:]\n",
    "    return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "\n",
    "def fetch_soil_for_points(points: List[Tuple[float, float]], batch_size: int = 200) -> pd.DataFrame:\n",
    "    \"\"\"Batch points to stay under SDA limits (<=100k rows) and avoid huge SQL.\n",
    "    Returns MUKEY + selected attributes for all points.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(points), batch_size):\n",
    "        batch = points[i : i + batch_size]\n",
    "        sql = build_point_query(batch)\n",
    "        df_batch = sda_query(sql)\n",
    "        df_batch[\"batch_index\"] = i // batch_size\n",
    "        results.append(df_batch)\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Example usage (small sample):\n",
    "# sample_points = [(41.58, -93.62), (40.75, -111.88)]  # (lat, lon)\n",
    "# df_sda = fetch_soil_for_points(sample_points, batch_size=50)\n",
    "# print(df_sda.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413229fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SDA request failed: HTTP 400\n<?xml version='1.0' encoding=\"UTF-8\" standalone=\"no\" ?>\r\n<ServiceExceptionReport xmlns=\"http://www.opengis.net/ogc\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/ogc http://schemas.opengis.net/wms/1.1.1/OGC-exception.xsd\">\n<ServiceException>\r\nInvalid query: Invalid object name &#39;SDA_Get_Mukey_from_intersect&#39;.</ServiceException>\r\n</ServiceExceptionReport>\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df_struct_out\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Example run (small batch):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m df_api = \u001b[43mfetch_soil_via_sda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTRUCT_COORDS_CSV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m df_api.to_csv(\u001b[33m\"\u001b[39m\u001b[33mgssurgo_enriched_via_api.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mfetch_soil_via_sda\u001b[39m\u001b[34m(struct_csv, batch_size)\u001b[39m\n\u001b[32m    142\u001b[39m     pts = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33mSTRUCT_ID\u001b[39m\u001b[33m\"\u001b[39m], batch[\u001b[33m\"\u001b[39m\u001b[33mLAT\u001b[39m\u001b[33m\"\u001b[39m], batch[\u001b[33m\"\u001b[39m\u001b[33mLON\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    143\u001b[39m     sql = build_full_sql(pts)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     df_batch = \u001b[43msda_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     results.append(df_batch)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36msda_query\u001b[39m\u001b[34m(sql)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp.ok:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Print server message to help diagnose (often includes SQL error)\u001b[39;00m\n\u001b[32m     58\u001b[39m     msg = resp.text[:\u001b[32m2000\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSDA request failed: HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m data = resp.json()\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "\u001b[31mRuntimeError\u001b[39m: SDA request failed: HTTP 400\n<?xml version='1.0' encoding=\"UTF-8\" standalone=\"no\" ?>\r\n<ServiceExceptionReport xmlns=\"http://www.opengis.net/ogc\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/ogc http://schemas.opengis.net/wms/1.1.1/OGC-exception.xsd\">\n<ServiceException>\r\nInvalid query: Invalid object name &#39;SDA_Get_Mukey_from_intersect&#39;.</ServiceException>\r\n</ServiceExceptionReport>\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Full pipeline via SDA API (no local GDB)\n",
    "# - Reads structure coordinates CSV\n",
    "# - Batches points, calls SDA post.rest with SQL\n",
    "# - Returns MUKEY + mapunit, dominant component, and depth-weighted horizon stats (0–100 cm)\n",
    "# ---------------------------------------------------------\n",
    "SDA_BATCH_SIZE = 200  # Tune to stay under SDA limits (<100k rows total)\n",
    "\n",
    "MU_COLS = [\n",
    "    \"brockdepmin\",\n",
    "    \"wtdepannmin\",\n",
    "    \"wtdepaprjunmin\",\n",
    "    \"slopegraddcp\",\n",
    "    \"drclassdcd\",\n",
    "    \"hydgrpdcd\",\n",
    "    \"flodfreqdcd\",\n",
    "    \"aws0100wta\",\n",
    "    \"aws0150wta\",\n",
    "]\n",
    "\n",
    "COMP_COLS = [\n",
    "    \"comppct_r\",\n",
    "    \"slope_r\",\n",
    "    \"drainagecl\",\n",
    "    \"hydricrating\",\n",
    "    \"hydgrp\",\n",
    "    \"corcon\",\n",
    "    \"corsteel\",\n",
    "    \"soilslippot\",\n",
    "    \"frostact\",\n",
    "    \"elev_r\",\n",
    "]\n",
    "\n",
    "CH_COLS = [\n",
    "    \"sandtotal_r\",\n",
    "    \"silttotal_r\",\n",
    "    \"claytotal_r\",\n",
    "    \"dbovendry_r\",\n",
    "    \"ksat_r\",\n",
    "    \"awc_r\",\n",
    "    \"om_r\",\n",
    "    \"ll_r\",\n",
    "    \"pi_r\",\n",
    "    \"wsatiated_r\",\n",
    "]\n",
    "\n",
    "\n",
    "def _values_clause(points: List[Tuple[int, float, float]]) -> str:\n",
    "    rows = []\n",
    "    for pid, lat, lon in points:\n",
    "        rows.append(\n",
    "            f\"({pid}, {lat}, {lon}, geometry::STPointFromText('POINT({lon} {lat})', 4326))\"\n",
    "        )\n",
    "    return \",\\n    \".join(rows)\n",
    "\n",
    "\n",
    "def build_full_sql(points: List[Tuple[int, float, float]]) -> str:\n",
    "    \"\"\"SQL to fetch mapunit + dominant component + horizon agg (0–100cm) for given points via SDA macro.\"\"\"\n",
    "    vals = _values_clause(points)\n",
    "    mu_select = \", \".join([f\"mu.{c}\" for c in MU_COLS])\n",
    "    comp_select = \", \".join([f\"cd.{c}\" for c in COMP_COLS])\n",
    "    ch_select = \", \".join([f\"ch.{c} AS ch_{c}\" for c in CH_COLS])\n",
    "    sql = f\"\"\"\n",
    "WITH pts AS (\n",
    "    SELECT * FROM (VALUES\n",
    "    {vals}\n",
    "    ) AS v(pid, lat, lon, geom)\n",
    "),\n",
    "hits AS (\n",
    "    SELECT p.pid, p.lat, p.lon, x.mukey\n",
    "    FROM pts p\n",
    "    CROSS APPLY SDA_Get_Mukey_from_intersect(p.geom) AS x\n",
    "),\n",
    "comp_dom AS (\n",
    "    SELECT mukey, cokey, {', '.join(COMP_COLS)},\n",
    "           ROW_NUMBER() OVER (PARTITION BY mukey ORDER BY comppct_r DESC) AS rn\n",
    "    FROM component\n",
    "),\n",
    "comp1 AS (\n",
    "    SELECT * FROM comp_dom WHERE rn = 1\n",
    "),\n",
    "ch AS (\n",
    "    SELECT cokey,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN sandtotal_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS sandtotal_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN silttotal_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS silttotal_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN claytotal_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS claytotal_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN dbovendry_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS dbovendry_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN ksat_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS ksat_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN awc_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS awc_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN om_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS om_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN ll_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS ll_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN pi_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS pi_r,\n",
    "           SUM(CASE WHEN eff_thk > 0 THEN wsatiated_r * eff_thk END) / NULLIF(SUM(NULLIF(eff_thk,0)),0) AS wsatiated_r\n",
    "    FROM (\n",
    "        SELECT cokey,\n",
    "               CASE\n",
    "                 WHEN hzdepb_r > 100 THEN 100 ELSE hzdepb_r\n",
    "               END - CASE\n",
    "                 WHEN hzdept_r < 0 THEN 0 ELSE hzdept_r\n",
    "               END AS eff_thk,\n",
    "               sandtotal_r, silttotal_r, claytotal_r, dbovendry_r, ksat_r, awc_r, om_r, ll_r, pi_r, wsatiated_r\n",
    "        FROM chorizon\n",
    "    ) c\n",
    "    WHERE eff_thk > 0\n",
    "    GROUP BY cokey\n",
    ")\n",
    "SELECT h.pid, h.lat, h.lon, h.mukey,\n",
    "       {mu_select},\n",
    "       comp1.cokey, {comp_select},\n",
    "       {ch_select}\n",
    "FROM hits h\n",
    "LEFT JOIN muaggatt mu ON mu.mukey = h.mukey\n",
    "LEFT JOIN comp1 ON comp1.mukey = h.mukey\n",
    "LEFT JOIN ch ON ch.cokey = comp1.cokey\n",
    "\"\"\"\n",
    "    return sql\n",
    "\n",
    "\n",
    "def fetch_soil_via_sda(struct_csv: str, batch_size: int = SDA_BATCH_SIZE) -> pd.DataFrame:\n",
    "    \"\"\"Fetch soil attributes for all structures via SDA post.rest.\n",
    "    - struct_csv: path to structure_coordinates.csv\n",
    "    - batch_size: number of points per SQL call (adjust if you hit limits)\n",
    "    \"\"\"\n",
    "    df_struct = pd.read_csv(struct_csv).copy()\n",
    "    if \"COORDINATES\" in df_struct.columns:\n",
    "        df_struct[\"COORDINATES\"] = df_struct[\"COORDINATES\"].apply(lambda x: x if isinstance(x, tuple) else ast.literal_eval(x))\n",
    "        df_struct[\"LAT\"] = df_struct[\"COORDINATES\"].apply(lambda c: c[0])\n",
    "        df_struct[\"LON\"] = df_struct[\"COORDINATES\"].apply(lambda c: c[1])\n",
    "    elif {\"LAT\", \"LON\"}.issubset(df_struct.columns):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Expected COORDINATES tuple column or LAT/LON columns in struct CSV\")\n",
    "\n",
    "    df_struct = df_struct.reset_index().rename(columns={\"index\": \"STRUCT_ID\"})\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(df_struct), batch_size):\n",
    "        batch = df_struct.iloc[i : i + batch_size]\n",
    "        pts = list(zip(batch[\"STRUCT_ID\"], batch[\"LAT\"], batch[\"LON\"]))\n",
    "        sql = build_full_sql(pts)\n",
    "        df_batch = sda_query(sql)\n",
    "        results.append(df_batch)\n",
    "\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_soil = pd.concat(results, ignore_index=True)\n",
    "    # Join back to original struct rows\n",
    "    df_struct_out = df_struct.merge(df_soil, left_on=\"STRUCT_ID\", right_on=\"pid\", how=\"left\")\n",
    "    return df_struct_out\n",
    "\n",
    "# Example run (small batch):\n",
    "df_api = fetch_soil_via_sda(STRUCT_COORDS_CSV, batch_size=100)\n",
    "df_api.to_csv(\"gssurgo_enriched_via_api.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a78bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb2e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
